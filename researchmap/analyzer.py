"""
ResearchMap APIé€£æºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ç ”ç©¶è€…ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¦AIåˆ†æã‚’å®Ÿè¡Œ
"""

import logging
import json
import re
import asyncio
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
import aiohttp
import collections
import itertools
from collections import Counter
try:
    from vertexai.generative_models import GenerativeModel
    from vertexai.language_models import TextGenerationModel
    VERTEX_AI_AVAILABLE = True
except ImportError:
    GenerativeModel = None
    TextGenerationModel = None
    VERTEX_AI_AVAILABLE = False

# --- ã‚°ãƒ©ãƒ•è§£æãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    import networkx as nx
    import community as community_louvain
    GRAPH_LIBS_AVAILABLE = True
except ImportError:
    GRAPH_LIBS_AVAILABLE = False
# --- ã“ã“ã¾ã§ ---

logger = logging.getLogger(__name__)

# æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
JAPANESE_STOPWORDS = {
    'ã«ã¤ã„ã¦', 'ã¨ã„ã†', 'ã§ã‚ã‚‹', 'ã¨ã—ã¦', 'ã«ãŠã„ã¦', 'ã«ã‚ˆã‚‹', 'ã«ã‚ˆã£ã¦',
    'ã—ã‹ã—', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'ã—ãŸãŒã£ã¦', 'ãã—ã¦', 'ãŠã‚ˆã³', 'ã“ã‚Œ', 'ãã‚Œ',
    'ç ”ç©¶', 'åˆ†æ', 'æ¤œè¨', 'å®Ÿé¨“', 'çµæœ', 'è€ƒå¯Ÿ', 'æ–¹æ³•', 'æ‰‹æ³•', 'æŠ€è¡“',
    'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿', 'è©•ä¾¡', 'é–‹ç™º', 'ææ¡ˆ', 'æ”¹å–„', 'åŠ¹æœ', 'å ±å‘Š',
    'ã¯ã˜ã‚ã«', 'ãŠã‚ã‚Šã«', 'ã¾ã¨ã‚', 'æ¦‚è¦', 'èƒŒæ™¯', 'ç›®çš„'
}

# è‹±èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
ENGLISH_STOPWORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
    'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',
    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',
    'does', 'did', 'we', 'i', 'you', 'he', 'she', 'it', 'they', 'this', 'that',
    'research', 'study', 'analysis', 'method', 'approach', 'system', 'data',
    'results', 'conclusion', 'introduction', 'paper', 'work'
}

# ç ”ç©¶åˆ†é‡ç‰¹æœ‰ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆ
TECHNICAL_KEYWORDS = {
    'AI', 'IOT', 'DX', 'ICT', 'IT', 'VR', 'AR', 'ML', 'NLP', 'CNN', 'RNN',
    'æ©Ÿæ¢°å­¦ç¿’', 'äººå·¥çŸ¥èƒ½', 'ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'æ·±å±¤å­¦ç¿’',
    'ãƒ™ã‚¤ã‚ºæœ€é©åŒ–', 'ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹'
}


class ResearchMapAnalyzer:
    """ResearchMap APIã‚’ä½¿ç”¨ã—ãŸç ”ç©¶è€…åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.api_base_url = "https://api.researchmap.jp"
        self.llm_model = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        if not VERTEX_AI_AVAILABLE:
            logger.warning("âš ï¸ VertexAI SDKãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ç°¡æ˜“åˆ†æãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œ")
            self.llm_model = None
            self.model_name = "simple_analysis"
            return
            
        try:
            # Gemini 2.0 Flash Liteã‚’å„ªå…ˆ
            self.llm_model = GenerativeModel("gemini-2.0-flash-lite-001")
            self.model_name = "gemini-2.0-flash-lite-001"
            logger.info(f"âœ… åˆ†æç”¨LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {self.model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Gemini 2.0 Flash LiteåˆæœŸåŒ–å¤±æ•—: {e}")
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.llm_model = TextGenerationModel.from_pretrained("text-bison@002")
                self.model_name = "text-bison@002"
                logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {self.model_name}")
            except Exception as e2:
                logger.error(f"âŒ LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e2}")
                self.llm_model = None
                self.model_name = "simple_analysis"
    
    def extract_researcher_id(self, researchmap_url: str) -> Optional[str]:
        """ResearchMap URLã‹ã‚‰ç ”ç©¶è€…IDã‚’æŠ½å‡º"""
        try:
            # URLãƒ‘ã‚¿ãƒ¼ãƒ³: https://researchmap.jp/{researcher_id}
            parsed_url = urlparse(researchmap_url)
            path_parts = parsed_url.path.strip('/').split('/')
            
            if path_parts and path_parts[0]:
                researcher_id = path_parts[0]
                logger.info(f"ğŸ“Œ ç ”ç©¶è€…IDæŠ½å‡º: {researcher_id}")
                return researcher_id
            
            return None
        except Exception as e:
            logger.error(f"âŒ ç ”ç©¶è€…IDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _fetch_all_papers(self, researcher_id: str, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®ç ”ç©¶è€…ã®å…¨è«–æ–‡ã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã—ã¦å–å¾—ã™ã‚‹"""
        all_papers = []
        start_index = 1
        limit = 100  # 1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å–å¾—ã™ã‚‹æœ€å¤§ä»¶æ•°
        headers = {"Accept": "application/json", "Accept-Language": "ja"}
        timeout = aiohttp.ClientTimeout(total=20)

        while True:
            papers_url = f"{self.api_base_url}/{researcher_id}/published_papers?start={start_index}&limit={limit}"
            try:
                async with session.get(papers_url, headers=headers, timeout=timeout) as response:
                    if response.status != 200:
                        logger.warning(f"ğŸ“„ è«–æ–‡å–å¾—ã§APIã‚¨ãƒ©ãƒ¼: status={response.status}, url={papers_url}")
                        break
                    
                    data = await response.json()
                    papers_on_page = data.get("items", [])
                    
                    if not papers_on_page:
                        logger.info("ğŸ“„ å…¨ã¦ã®è«–æ–‡ã‚’å–å¾—å®Œäº†ã€‚")
                        break
                    
                    all_papers.extend(papers_on_page)
                    logger.info(f"ğŸ“„ è«–æ–‡ã‚’{len(papers_on_page)}ä»¶å–å¾— (åˆè¨ˆ: {len(all_papers)}ä»¶)")
                    
                    if len(papers_on_page) < limit:
                        logger.info("ğŸ“„ ã“ã‚ŒãŒæœ€çµ‚ãƒšãƒ¼ã‚¸ã§ã™ã€‚")
                        break
                        
                    start_index += limit
                    await asyncio.sleep(0.5) # APIã¸ã®è² è·ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã®å¾…æ©Ÿ

            except asyncio.TimeoutError:
                logger.warning(f"ğŸ“„ è«–æ–‡å–å¾—ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {papers_url}")
                break
            except Exception as e:
                logger.error(f"ğŸ“„ è«–æ–‡å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        return all_papers

    async def _fetch_all_misc(self, researcher_id: str, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®ç ”ç©¶è€…ã®å…¨ãã®ä»–æ¥­ç¸¾(misc)ã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã—ã¦å–å¾—ã™ã‚‹"""
        all_misc = []
        start_index = 1
        limit = 100
        headers = {"Accept": "application/json", "Accept-Language": "ja"}
        timeout = aiohttp.ClientTimeout(total=20)

        while True:
            misc_url = f"{self.api_base_url}/{researcher_id}/misc?start={start_index}&limit={limit}"
            try:
                async with session.get(misc_url, headers=headers, timeout=timeout) as response:
                    if response.status != 200:
                        logger.warning(f"ğŸ“š ãã®ä»–æ¥­ç¸¾å–å¾—ã§APIã‚¨ãƒ©ãƒ¼: status={response.status}, url={misc_url}")
                        break
                    
                    data = await response.json()
                    misc_on_page = data.get("items", [])
                    
                    if not misc_on_page:
                        logger.info("ğŸ“š å…¨ã¦ã®ãã®ä»–æ¥­ç¸¾ã‚’å–å¾—å®Œäº†ã€‚")
                        break
                    
                    all_misc.extend(misc_on_page)
                    logger.info(f"ğŸ“š ãã®ä»–æ¥­ç¸¾ã‚’{len(misc_on_page)}ä»¶å–å¾— (åˆè¨ˆ: {len(all_misc)}ä»¶)")
                    
                    if len(misc_on_page) < limit:
                        logger.info("ğŸ“š ã“ã‚ŒãŒæœ€çµ‚ãƒšãƒ¼ã‚¸ã§ã™ã€‚")
                        break
                        
                    start_index += limit
                    await asyncio.sleep(0.5)

            except asyncio.TimeoutError:
                logger.warning(f"ğŸ“š ãã®ä»–æ¥­ç¸¾å–å¾—ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {misc_url}")
                break
            except Exception as e:
                logger.error(f"ğŸ“š ãã®ä»–æ¥­ç¸¾å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        return all_misc

    async def fetch_researcher_data(self, researcher_id: str) -> Optional[Dict[str, Any]]:
        """ResearchMap APIã‹ã‚‰ç ”ç©¶è€…æƒ…å ±ã‚’å–å¾—ã—ã€å…¨è«–æ–‡æƒ…å ±ã‚‚å–å¾—ã™ã‚‹"""
        try:
            async with aiohttp.ClientSession() as session:
                # åŸºæœ¬æƒ…å ±ã®å–å¾—
                profile_url = f"{self.api_base_url}/{researcher_id}"
                headers = {"Accept": "application/json", "Accept-Language": "ja"}
                timeout = aiohttp.ClientTimeout(total=15)
                
                async with session.get(profile_url, headers=headers, timeout=timeout) as response:
                    if response.status != 200:
                        logger.error(f"âŒ ResearchMap API ã‚¨ãƒ©ãƒ¼: {response.status}")
                        if response.status == 404 or response.status >= 500:
                            logger.info("ğŸ”„ ResearchMap APIåˆ©ç”¨ä¸å¯ã®ãŸã‚ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
                            return self._create_mock_researcher_data(researcher_id)
                        return None
                    
                    data = await response.json()
                    researcher_data = self._parse_researcher_data(data)
                    
                    # å…¨è«–æ–‡ã‚’å–å¾—
                    logger.info(f"ğŸ“„ {researcher_id} ã®å…¨è«–æ–‡å–å¾—ã‚’é–‹å§‹...")
                    all_papers = await self._fetch_all_papers(researcher_id, session)
                    researcher_data["papers"] = all_papers if all_papers else researcher_data.get("papers", [])
                    if not all_papers:
                        logger.warning("âš ï¸ å…¨è«–æ–‡ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åŸºæœ¬æƒ…å ±ã«å«ã¾ã‚Œã‚‹è«–æ–‡ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

                    # å…¨ãã®ä»–æ¥­ç¸¾(misc)ã‚’å–å¾—
                    logger.info(f"ğŸ“š {researcher_id} ã®å…¨ãã®ä»–æ¥­ç¸¾å–å¾—ã‚’é–‹å§‹...")
                    all_misc = await self._fetch_all_misc(researcher_id, session)

                    # è«–æ–‡ã¨ãã®ä»–æ¥­ç¸¾ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    paper_count = len(researcher_data.get("papers", []))
                    misc_count = 0

                    if all_misc:
                        logger.info(f"âœ… å…¨{len(all_misc)}ä»¶ã®ãã®ä»–æ¥­ç¸¾ã‚’å–å¾—å®Œäº†ã€‚")
                        # è«–æ–‡ã¨ãã®ä»–æ¥­ç¸¾ã§ã‚­ãƒ¼ãŒé‡è¤‡ã™ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€idã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
                        existing_paper_ids = {p.get("@id") for p in researcher_data["papers"]}
                        unique_misc = [m for m in all_misc if m.get("@id") not in existing_paper_ids]
                        misc_count = len(unique_misc)
                        # æ¥­ç¸¾ãƒªã‚¹ãƒˆã«ãã®ä»–æ¥­ç¸¾ã‚’è¿½åŠ 
                        researcher_data["papers"].extend(unique_misc)
                        logger.info(f"âœ… ãã®ä»–æ¥­ç¸¾{misc_count}ä»¶ã‚’æ¥­ç¸¾ãƒªã‚¹ãƒˆã«è¿½åŠ ã€‚")
                    else:
                        logger.warning("âš ï¸ ãã®ä»–æ¥­ç¸¾ã¯å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

                    # ã‚«ã‚¦ãƒ³ãƒˆã‚’ä¿å­˜
                    researcher_data["paper_count"] = paper_count
                    researcher_data["misc_count"] = misc_count
                    logger.info(f"ğŸ“Š ã‚«ã‚¦ãƒ³ãƒˆçµæœ: è«–æ–‡={paper_count}ä»¶, ãã®ä»–æ¥­ç¸¾={misc_count}ä»¶, åˆè¨ˆ={len(researcher_data['papers'])}ä»¶")

                    return researcher_data
                    
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ ResearchMap APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            return self._create_mock_researcher_data(researcher_id)
        except Exception as e:
            logger.error(f"âŒ ResearchMap APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ğŸ”„ ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            return self._create_mock_researcher_data(researcher_id)
    
    def _parse_researcher_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ResearchMap APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        # åŸºæœ¬ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ï¼ˆ@graphã®å¤–å´ï¼‰
        profile_data = {
            "user_id": data.get("rm:user_id"),
            "permalink": data.get("permalink"),
            "family_name": data.get("family_name", {}),
            "given_name": data.get("given_name", {}),
            "display_name": data.get("display_name", {}),
            "image": data.get("image"),
            "affiliations": data.get("affiliations", []),
            "degrees": data.get("degrees", []),
            "see_also": data.get("see_also", []),
            "identifiers": data.get("identifiers", {})
        }
        
        # @graphå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¨®é¡åˆ¥ã«æ•´ç†
        papers = []
        projects = []
        awards = []
        research_interests = []
        research_areas = []
        presentations = []
        misc_publications = []
        industrial_properties = []
        
        # @graphãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å„ã‚¿ã‚¤ãƒ—ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        if "@graph" in data:
            for graph_item in data.get("@graph", []):
                item_type = graph_item.get("@type")
                items = graph_item.get("items", [])
                
                if item_type == "research_projects":
                    projects.extend(items)
                    logger.info(f"ğŸ”¬ ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "awards":
                    awards.extend(items)
                    logger.info(f"ğŸ† å—è³æ­´ {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "research_interests":
                    research_interests.extend(items)
                    logger.info(f"ğŸ’¡ ç ”ç©¶é–¢å¿ƒ {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "research_areas":
                    research_areas.extend(items)
                    logger.info(f"ğŸ¯ ç ”ç©¶é ˜åŸŸ {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "presentations":
                    presentations.extend(items)
                    logger.info(f"ğŸ¤ ç™ºè¡¨ {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "misc":
                    misc_publications.extend(items)
                    logger.info(f"ğŸ“š ãã®ä»–å‡ºç‰ˆç‰© {len(items)}ä»¶ã‚’æŠ½å‡º")
                    
                elif item_type == "industrial_property_rights":
                    industrial_properties.extend(items)
                    logger.info(f"ğŸ’¼ ç‰¹è¨± {len(items)}ä»¶ã‚’æŠ½å‡º")
        
        # æ—§å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ã‚‚å¯¾å¿œï¼ˆ@graphãŒãªã„å ´åˆï¼‰
        else:
            if "published_papers" in data:
                papers = data.get("published_papers", [])
            if "research_projects" in data:
                projects = data.get("research_projects", [])
            if "awards" in data:
                awards = data.get("awards", [])
        
        return {
            "profile": profile_data,
            "papers": papers,
            "projects": projects,
            "awards": awards,
            "research_interests": research_interests,
            "research_areas": research_areas,
            "presentations": presentations,
            "misc_publications": misc_publications,
            "industrial_properties": industrial_properties
        }
    
    async def analyze_researcher(
        self, 
        researchmap_url: str, 
        query: str,
        basic_info: Optional[Dict[str, Any]] = None,
        include_keyword_map: bool = False
    ) -> Dict[str, Any]:
        """ç ”ç©¶è€…ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ"""
        
        # ç ”ç©¶è€…IDã®æŠ½å‡º
        researcher_id = self.extract_researcher_id(researchmap_url)
        if not researcher_id:
            logger.error("âŒ ç ”ç©¶è€…IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“")
            return self._create_error_response("ç„¡åŠ¹ãªResearchMap URLã§ã™")
        
        # ResearchMapã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        researcher_data = await self.fetch_researcher_data(researcher_id)
        if not researcher_data:
            logger.error("âŒ ResearchMapã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            return self._create_error_response("ResearchMapã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # AIåˆ†æã®å®Ÿè¡Œ
        analysis = await self._perform_ai_analysis(researcher_data, query, basic_info, include_keyword_map)
        
        return {
            "status": "success",
            "analysis": analysis
        }
    
    async def _perform_ai_analysis(
        self, 
        researcher_data: Dict[str, Any], 
        query: str,
        basic_info: Optional[Dict[str, Any]] = None,
        include_keyword_map: bool = False
    ) -> Dict[str, Any]:
        """AIã«ã‚ˆã‚‹ç ”ç©¶è€…åˆ†æ"""
        logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—ç”Ÿæˆãƒ•ãƒ©ã‚°: {include_keyword_map}")
        
        profile = researcher_data.get("profile", {})
        papers = researcher_data.get("papers", [])
        projects = researcher_data.get("projects", [])
        awards = researcher_data.get("awards", [])
        research_interests = researcher_data.get("research_interests", [])
        research_areas = researcher_data.get("research_areas", [])
        presentations = researcher_data.get("presentations", [])
        industrial_properties = researcher_data.get("industrial_properties", [])
        
        # åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
        researcher_name = self._get_name(profile)
        affiliation = self._get_affiliation(profile)
        
        # çµ±è¨ˆæƒ…å ±
        paper_count = researcher_data.get("paper_count", 0)
        misc_count = researcher_data.get("misc_count", 0)
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚«ã‚¦ãƒ³ãƒˆãŒãªã„å ´åˆã¯è«–æ–‡ãƒªã‚¹ãƒˆã®å…¨é•·ã‚’è«–æ–‡æ•°ã¨ã™ã‚‹
        if paper_count == 0 and misc_count == 0:
            total_papers = len(papers)
        else:
            total_papers = paper_count

        total_achievements = len(papers) # ã“ã‚Œã¯è«–æ–‡ï¼‹ãã®ä»–æ¥­ç¸¾ã®åˆè¨ˆ
        total_projects = len(projects)
        total_awards = len(awards)
        total_presentations = len(presentations)
        total_patents = len(industrial_properties)
        
        # ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º
        keywords = self._extract_research_keywords(research_interests, research_areas)
        
        # é–¢é€£ã™ã‚‹ä¸Šä½è«–æ–‡ã®æŠ½å‡º
        relevant_papers = self._extract_relevant_papers(papers, query, limit=5)
        
        # ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æŠ½å‡º
        key_projects = self._extract_key_projects(projects, query, limit=3)
        
        # æœ€è¿‘ã®ç™ºè¡¨ã®æŠ½å‡º
        recent_presentations = self._extract_recent_presentations(presentations, limit=3)
        
        # ä¸»è¦ç‰¹è¨±ã®æŠ½å‡º
        key_patents = self._extract_key_patents(industrial_properties, query, limit=3)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã®å®Ÿè¡Œ
        keyword_analysis = None
        if include_keyword_map:
            keyword_analysis = await self._create_keyword_analysis(
                researcher_name, papers, projects, research_interests, research_areas
            )

        # ã‚¹ã‚³ã‚¢è¨ˆç®—ã¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        scores = {}
        detailed_analysis = ""
        
        if self.llm_model and self.model_name != "simple_analysis":
            scores = await self._calculate_scores(
                profile, papers, projects, awards, query,
                research_interests, research_areas
            )
            # AIã«ã‚ˆã‚‹è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            detailed_analysis = await self._generate_detailed_analysis(
                profile,
                paper_count,
                misc_count,
                papers, 
                projects, 
                awards, 
                query,
                research_interests, 
                presentations, 
                industrial_properties
            )
        else:
            logger.info("ğŸ”„ LLMåˆ©ç”¨ä¸å¯ã®ãŸã‚ç°¡æ˜“åˆ†æãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨")
            scores = self._calculate_simple_scores(
                total_achievements, total_projects, total_awards, query, papers
            )
            detailed_analysis = self._generate_simple_analysis(
                researcher_name, query, total_achievements, total_projects
            )
        
        return {
            "researcher_name": researcher_name,
            "affiliation": affiliation,
            "scores": scores,
            "research_keywords": keywords,
            "total_papers": total_achievements, # UIè¡¨ç¤ºç”¨ã«åˆè¨ˆå€¤ã‚’ç¶­æŒ
            "total_projects": total_projects,
            "total_awards": total_awards,
            "total_presentations": total_presentations,
            "total_patents": total_patents,
            "detailed_analysis": detailed_analysis,
            "top_papers": relevant_papers,
            "key_projects": key_projects,
            "recent_presentations": recent_presentations,
            "key_patents": key_patents,
            "keyword_analysis": keyword_analysis
        }
    
    def _get_name(self, profile: Dict) -> str:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰åå‰ã‚’å–å¾—"""
        # display_nameãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
        if "display_name" in profile:
            display_name = profile.get("display_name", {})
            if isinstance(display_name, dict) and display_name.get("ja"):
                return display_name.get("ja")
            elif isinstance(display_name, dict) and display_name.get("en"):
                return display_name.get("en")
        
        # family_nameã¨given_nameã‹ã‚‰æ§‹ç¯‰
        family_name = profile.get("family_name", {})
        given_name = profile.get("given_name", {})
        
        # æ—¥æœ¬èªå
        if family_name.get("ja") and given_name.get("ja"):
            return f"{family_name['ja']} {given_name['ja']}"
        
        # è‹±èªå
        if family_name.get("en") and given_name.get("en"):
            return f"{given_name['en']} {family_name['en']}"
        
        return "åå‰ä¸æ˜"
    
    def _get_affiliation(self, profile: Dict) -> str:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰æ‰€å±ã‚’å–å¾—"""
        # ç¾åœ¨ã®æ‰€å±æƒ…å ±
        if "affiliation" in profile:
            affiliation = profile.get("affiliation", {})
            # æ—¥æœ¬èªå
            if isinstance(affiliation, dict) and "ja" in affiliation:
                return affiliation.get("ja", "")
            # è‹±èªå
            if isinstance(affiliation, dict) and "en" in affiliation:
                return affiliation.get("en", "")
            # æ–‡å­—åˆ—ã®å ´åˆ
            if isinstance(affiliation, str):
                return affiliation
        
        # affiliationsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆè¤‡æ•°å½¢ï¼‰
        if "affiliations" in profile:
            affiliations = profile.get("affiliations", [])
            if affiliations and len(affiliations) > 0:
                latest = affiliations[0]
                if isinstance(latest, dict):
                    return latest.get("affiliation", {}).get("ja", latest.get("affiliation", {}).get("en", ""))
        
        return "æ‰€å±ä¸æ˜"
    
    def _extract_relevant_papers(self, papers: List[Dict], query: str, limit: int = 5) -> List[Dict]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹è«–æ–‡ã‚’æŠ½å‡º"""
        relevant_papers = []
        query_lower = query.lower()
        
        for paper in papers:
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—ï¼ˆResearchMapã®å®Ÿéš›ã®æ§‹é€ ã«åˆã‚ã›ã‚‹ï¼‰
            # paper_title ã¾ãŸã¯ published_paper_title ã®ä¸¡æ–¹ã«å¯¾å¿œ
            paper_title_data = paper.get("paper_title") or paper.get("published_paper_title", {})
            
            if isinstance(paper_title_data, dict):
                title_ja = paper_title_data.get("ja", "")
                title_en = paper_title_data.get("en", "")
            else:
                title_ja = ""
                title_en = ""
            
            # æ—§å½¢å¼ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ç¢ºèª
            if not title_ja and "titles" in paper:
                for title in paper.get("titles", []):
                    if title.get("lang") == "ja":
                        title_ja = title.get("title", "")
                    elif title.get("lang") == "en":
                        title_en = title.get("title", "")
            
            # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
            relevance_score = 0
            if query_lower in title_ja.lower():
                relevance_score += 2
            if query_lower in title_en.lower():
                relevance_score += 1
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            for keyword in query_lower.split():
                if keyword in title_ja.lower() or keyword in title_en.lower():
                    relevance_score += 0.5
            
            if relevance_score > 0 or len(relevant_papers) < 2:  # æœ€ä½2ä»¶ã¯å«ã‚ã‚‹
                # å¹´ã®å–å¾—
                year = paper.get("publication_date", "ä¸æ˜")
                if isinstance(year, dict):
                    year = year.get("year", "ä¸æ˜")
                elif isinstance(year, str) and len(year) >= 4:
                    year = year[:4]
                
                # ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åã®å–å¾—
                journal = ""
                if "identifiers" in paper:
                    journal = paper.get("identifiers", {}).get("misc", [])
                    if isinstance(journal, list) and len(journal) > 0:
                        journal = journal[0]
                    else:
                        journal = ""
                
                if not journal and "misc" in paper:
                    journal = paper.get("misc", "")
                
                relevant_papers.append({
                    "year": year,
                    "title_ja": title_ja,
                    "title_en": title_en,
                    "journal": journal,
                    "relevance_score": relevance_score
                })
        
        # é–¢é€£æ€§ã§ã‚½ãƒ¼ãƒˆ
        relevant_papers.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        # ã‚¹ã‚³ã‚¢ã‚’å‰Šé™¤ã—ã¦è¿”ã™
        for paper in relevant_papers[:limit]:
            paper.pop("relevance_score", None)
        
        return relevant_papers[:limit]
    
    def _extract_key_projects(self, projects: List[Dict], query: str, limit: int = 3) -> List[Dict]:
        """ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æŠ½å‡º"""
        key_projects = []
        
        for i, project in enumerate(projects[:limit]):  # æœ€æ–°ã®ã‚‚ã®ã‹ã‚‰
            # ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æ§‹é€ ã‚’ç¢ºèª
            logger.info(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ{i+1}ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {json.dumps(project, ensure_ascii=False, indent=2)[:500]}...")
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾— - ResearchMap APIã®å®Ÿéš›ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«å¯¾å¿œ
            # research_project_title ã¾ãŸã¯ project_title ã®ä¸¡æ–¹ã«å¯¾å¿œ
            project_title = project.get("research_project_title") or project.get("project_title", {})
            logger.info(f"project_titleå–å¾—çµæœ: {project_title}, å‹: {type(project_title)}")
            
            # project_titleãŒæ–‡å­—åˆ—ã®å ´åˆã¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã«å¯¾å¿œ
            if isinstance(project_title, str):
                title = project_title
                logger.info(f"æ–‡å­—åˆ—å‹ã®ã‚¿ã‚¤ãƒˆãƒ«: {title}")
            elif isinstance(project_title, dict):
                title_ja = project_title.get("ja")
                title_en = project_title.get("en")
                logger.info(f"è¾æ›¸å‹ã®ã‚¿ã‚¤ãƒˆãƒ« - ja: {repr(title_ja)}, en: {repr(title_en)}")
                
                # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’å„ªå…ˆ
                if title_ja is not None and title_ja != "":
                    title = title_ja
                elif title_en is not None and title_en != "":
                    title = title_en
                else:
                    title = "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                    logger.warning(f"ã‚¿ã‚¤ãƒˆãƒ«ãŒå–å¾—ã§ãã¾ã›ã‚“: {project_title}")
            else:
                title = "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                logger.warning(f"äºˆæœŸã—ãªã„project_titleã®å‹: {type(project_title)}, å€¤: {project_title}")
            
            # æœŸé–“ã®å–å¾—
            from_date = project.get("from_date", "")
            to_date = project.get("to_date", "")
            
            # å¹´ã‚’æŠ½å‡º
            start_year = "?"
            end_year = "ç¶™ç¶šä¸­"
            
            if from_date:
                if isinstance(from_date, str) and len(from_date) >= 4:
                    start_year = from_date[:4]
                elif isinstance(from_date, dict):
                    start_year = from_date.get("year", "?")
            
            if to_date:
                if isinstance(to_date, str) and len(to_date) >= 4:
                    end_year = to_date[:4]
                elif isinstance(to_date, dict):
                    end_year = to_date.get("year", "ç¶™ç¶šä¸­")
            
            period = f"{start_year}-{end_year}"
            
            # æœ€çµ‚çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã®ç¢ºèª
            final_title = str(title) if title else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
            logger.info(f"æœ€çµ‚ã‚¿ã‚¤ãƒˆãƒ«: {final_title}")
            
            key_projects.append({
                "title": final_title,
                "period": period
            })
        
        return key_projects
    
    def _extract_research_keywords(self, research_interests: List[Dict], research_areas: List[Dict]) -> List[str]:
        """ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # ç ”ç©¶é–¢å¿ƒã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        for interest in research_interests:
            keyword_dict = interest.get("keyword", {})
            if keyword_dict.get("ja"):
                keywords.append(keyword_dict["ja"])
            elif keyword_dict.get("en"):
                keywords.append(keyword_dict["en"])
        
        # ç ”ç©¶é ˜åŸŸã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        for area in research_areas:
            research_field = area.get("research_field", {})
            if research_field.get("ja"):
                keywords.append(research_field["ja"])
            
            research_keyword = area.get("research_keyword", {})
            if isinstance(research_keyword, dict) and research_keyword.get("ja"):
                keywords.append(research_keyword["ja"])
            elif isinstance(research_keyword, str) and research_keyword:
                # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†å‰²
                keywords.extend([k.strip() for k in research_keyword.split("ã€")])
        
        # é‡è¤‡ã‚’é™¤å»
        return list(dict.fromkeys(keywords))

    def _tokenize_text(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»"""
        tokens = re.findall(r'\b\w+\b', text.lower())
        filtered_tokens = [
            token for token in tokens 
            if token not in JAPANESE_STOPWORDS and 
               token not in ENGLISH_STOPWORDS and 
               len(token) > 1 # 1æ–‡å­—ã®å˜èªã¯é™¤å¤–
        ]
        return filtered_tokens

    # --- â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ä¿®æ­£ãƒ»è¿½åŠ ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ â–¼â–¼â–¼ ---
    
    async def _create_keyword_analysis(
        self,
        researcher_name: str,
        papers: List[Dict],
        projects: List[Dict],
        research_interests: List[Dict],
        research_areas: List[Dict]
    ) -> Dict[str, Any]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†æã‚’è¡Œã„ã€å­¦å•é ˜åŸŸã€ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã€LLMã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ï¼ˆæœ€çµ‚ç¢ºå®šç‰ˆï¼‰
        """
        base_result = {
            "academic_fields": [],
            "research_trends": {},
            "word_cloud": [],
            "llm_comment": "LLMã«ã‚ˆã‚‹åˆ†æã‚³ãƒ¡ãƒ³ãƒˆã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        }

        if not self.llm_model:
            logger.warning("âš ï¸ LLMãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            base_result["llm_comment"] = "LLMãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€è©³ç´°ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã¯ã§ãã¾ã›ã‚“ã€‚"
            return base_result

        try:
            all_texts = []
            paper_info = [] # (title, year) ã®ã‚¿ãƒ—ãƒ«ã‚’æ ¼ç´

            for paper in papers:
                title_data = paper.get("paper_title") or paper.get("published_paper_title", {})
                title_text = ""
                if isinstance(title_data, dict):
                    title_text = title_data.get("ja", "") or title_data.get("en", "")
                
                if title_text: 
                    all_texts.append(title_text)
                    year_str = paper.get("publication_date", "")
                    year = None
                    if isinstance(year_str, dict):
                        year_str = year_str.get("year", "")
                    if isinstance(year_str, str) and len(year_str) >= 4 and year_str[:4].isdigit():
                        year = int(year_str[:4])
                    if year:
                        paper_info.append((title_text, year))

            for project in projects:
                title_data = project.get("research_project_title") or project.get("project_title", {})
                if isinstance(title_data, str):
                    all_texts.append(title_data)
                elif isinstance(title_data, dict):
                    all_texts.append(title_data.get("ja", "") or title_data.get("en", ""))
            
            all_texts.extend(self._extract_research_keywords(research_interests, research_areas))

            if not all_texts:
                base_result["llm_comment"] = "åˆ†æã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                return base_result

            combined_text = "\n".join(filter(None, all_texts))

            # 1. å­¦å•é ˜åŸŸã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚’LLMã§ç”Ÿæˆ
            prompt1 = f"""ç ”ç©¶è€…ã€Œ{researcher_name}ã€æ°ã®ä»¥ä¸‹ã®ç ”ç©¶æ´»å‹•ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€ä¸»è¦ãªå­¦å•é ˜åŸŸã€é »å‡ºå˜èªã€ãã—ã¦ç·è©•ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:
{combined_text}

å‡ºåŠ›ã¯ä»¥ä¸‹ã®å½¢å¼ã§ã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ `###` ã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚

### ACADEMIC_FIELDS
å­¦å•é ˜åŸŸ1; ç°¡å˜ãªèª¬æ˜1
å­¦å•é ˜åŸŸ2; ç°¡å˜ãªèª¬æ˜2

### WORD_CLOUD
é »å‡ºå˜èª1; 95
Frequent Word 2; 80

### LLM_COMMENT
ã“ã“ã‹ã‚‰ç·è©•ã‚³ãƒ¡ãƒ³ãƒˆã‚’é–‹å§‹ã€‚{researcher_name}æ°ã®å°‚é–€æ€§ã‚„ä¸»è¦ãªç ”ç©¶ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€åŒæ°ã®ç ”ç©¶æ´»å‹•ã‚’è©•ä¾¡ã™ã‚‹å½¢ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
"""
            llm_response_text = await self._generate_llm_response(prompt1, max_tokens=1500)
            if not llm_response_text:
                base_result["llm_comment"] = "LLMã‹ã‚‰ã®å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                return base_result

            analysis = self._parse_llm_text_response(llm_response_text)

            # 2. ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—
            if analysis["academic_fields"] and paper_info:
                # è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã¨å­¦å•é ˜åŸŸã®é–¢é€£ä»˜ã‘ã‚’LLMã«ä¾é ¼
                field_names = [f["field"] for f in analysis["academic_fields"]]
                paper_titles = [p[0] for p in paper_info]
                
                prompt2 = f"""ä»¥ä¸‹ã®è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆã‚’ã€æŒ‡å®šã•ã‚ŒãŸå­¦å•é ˜åŸŸã®ã„ãšã‚Œã‹ä¸€ã¤ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

å­¦å•é ˜åŸŸãƒªã‚¹ãƒˆ:
{', '.join(field_names)}

è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆ:
{', '.join(paper_titles)}

å‡ºåŠ›å½¢å¼ã¯ã€Œè«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«; å­¦å•é ˜åŸŸã€ã®å½¢å¼ã§ã€ä¸€è¡Œã«ä¸€ã¤ãšã¤è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ä¾‹:
è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«A; å­¦å•é ˜åŸŸ1
è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«B; å­¦å•é ˜åŸŸ2
"""
                categorization_response = await self._generate_llm_response(prompt2, max_tokens=2000)
                
                paper_to_field = {}
                for line in categorization_response.split('\n'):
                    parts = [p.strip() for p in line.split(';', 1)]
                    if len(parts) == 2:
                        paper_to_field[parts[0]] = parts[1]

                # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—
                papers_by_year = Counter(p[1] for p in paper_info)
                field_papers_by_year = collections.defaultdict(lambda: collections.defaultdict(int))

                for title, year in paper_info:
                    field = paper_to_field.get(title)
                    if field in field_names:
                        field_papers_by_year[field][year] += 1
                
                calculated_trends = {}
                for field_name, year_counts in field_papers_by_year.items():
                    calculated_trends[field_name] = {}
                    for year in sorted(year_counts.keys()):
                        if papers_by_year.get(year, 0) > 0:
                            calculated_trends[field_name][str(year)] = round(year_counts[year] / papers_by_year[year], 2)
                
                analysis["research_trends"] = calculated_trends

            return analysis

        except Exception as e:
            logger.error(f"âŒ LLMã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            base_result["llm_comment"] = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            return base_result

    def _parse_llm_text_response(self, text: str) -> Dict[str, Any]:
        """LLMã®ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¾æ›¸ã«å¤‰æ›ã™ã‚‹"""
        analysis = {
            "academic_fields": [],
            "word_cloud": [],
            "llm_comment": "ã‚³ãƒ¡ãƒ³ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
            "research_trends": {}
        }

        sections = {
            "ACADEMIC_FIELDS": [],
            "WORD_CLOUD": [],
            "LLM_COMMENT": []
        }
        current_section = None

        for line in text.split('\n'):
            line = line.strip()
            if line.startswith("###"):
                current_section = line.replace("###", "").strip()
            elif current_section and line:
                if current_section in sections:
                    sections[current_section].append(line)
        
        # Academic Fields
        for line in sections["ACADEMIC_FIELDS"]:
            parts = [p.strip() for p in line.split(';', 1)]
            if len(parts) == 2:
                analysis["academic_fields"].append({"field": parts[0], "description": parts[1]})

        # Word Cloud
        for line in sections["WORD_CLOUD"]:
            parts = [p.strip() for p in line.split(';', 1)]
            if len(parts) == 2:
                try:
                    size = int(parts[1])
                    analysis["word_cloud"].append({"text": parts[0], "size": size})
                except ValueError:
                    logger.warning(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ã‚µã‚¤ã‚ºå¤‰æ›ã«å¤±æ•—: {parts[1]}")

        # LLM Comment
        if sections["LLM_COMMENT"]:
            analysis["llm_comment"] = "\n".join(sections["LLM_COMMENT"])

        return analysis

    def _extract_json_from_response(self, text: str) -> Optional[str]:
        """LLMã®å¿œç­”ã‹ã‚‰JSONæ–‡å­—åˆ—ã‚’æŠ½å‡ºã™ã‚‹"""
        # 1. ```json ... ``` ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
        match = re.search(r"```(?:json)?\s*({.*?})\s*```", text, re.DOTALL)
        if match:
            return match.group(1)
        
        # 2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã€æœ€åˆã® '{' ã‹ã‚‰æœ€å¾Œã® '}' ã¾ã§ã‚’æŠ½å‡º
        start_index = text.find('{')
        end_index = text.rfind('}')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            return text[start_index:end_index+1]
            
        return None

    def _fix_malformed_json(self, json_string: str) -> str:
        """
        LLMãŒç”Ÿæˆã—ãŒã¡ãªä¸æ­£ãªJSONã‚’ä¿®æ­£ã™ã‚‹ï¼ˆå …ç‰¢ãªè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰ˆï¼‰
        """
        # 1. æœ«å°¾ã®ã‚«ãƒ³ãƒã‚’å‰Šé™¤ (ä¾‹: [1, 2,])
        fixed_string = re.sub(r",\s*([}\]])", r"\1", json_string)

        # 2. æ¬ è½ã—ã¦ã„ã‚‹ã‚«ãƒ³ãƒã‚’è¿½åŠ ã™ã‚‹
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: (å€¤ã®çµ‚ã‚ã‚Š)(ç©ºç™½)(æ¬¡ã®ã‚­ãƒ¼ã®å§‹ã¾ã‚Š) -> (å€¤ã®çµ‚ã‚ã‚Š),(ç©ºç™½)(æ¬¡ã®ã‚­ãƒ¼ã®å§‹ã¾ã‚Š)
        # å€¤ã®çµ‚ã‚ã‚Š: ", }, ], true, false, ã¾ãŸã¯æ•°å­—
        # ç©ºç™½: \s+ (æ”¹è¡Œã‚’å«ã‚€)
        # æ¬¡ã®ã‚­ãƒ¼ã®å§‹ã¾ã‚Š: "
        
        # æ–‡å­—åˆ—ã®å¾Œ: " ... " "..." -> " ... ", "..."
        fixed_string = re.sub(r'("\s*)\n(\s*")', r'\1,\n\2', fixed_string)
        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¾Œ: } "..." -> }, "..."
        fixed_string = re.sub(r'(\}\s*)\n(\s*")', r'\1,\n\2', fixed_string)
        # é…åˆ—ã®å¾Œ: ] "..." -> ], "..."
        fixed_string = re.sub(r'(\]\s*)\n(\s*")', r'\1,\n\2', fixed_string)
        # æ•°å€¤ã®å¾Œ: 123 "..." -> 123, "..."
        fixed_string = re.sub(r'(\d\s*)\n(\s*")', r'\1,\n\2', fixed_string)
        # ãƒ–ãƒ¼ãƒ«å€¤/nullã®å¾Œ: true "..." -> true, "..."
        fixed_string = re.sub(r'(true|false|null)(\s*)\n(\s*")', r'\1,\2\n\3', fixed_string)

        return fixed_string

    async def _generate_llm_response(self, prompt: str, max_tokens: int = 200) -> str:
        """LLMå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹å…±é€šé–¢æ•°"""
        if not self.llm_model:
            return ""
        try:
            if "gemini" in self.model_name:
                response = await self.llm_model.generate_content_async(
                    prompt,
                    generation_config={"temperature": 0.2, "max_output_tokens": max_tokens, "top_p": 0.8}
                )
                return response.text.strip()
            else:
                response = self.llm_model.predict(
                    prompt,
                    temperature=0.2,
                    max_output_tokens=max_tokens,
                    top_p=0.8
                )
                return response.text.strip()
        except Exception as e:
            logger.error(f"LLMå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _extract_key_patents(self, industrial_properties: List[Dict], query: str, limit: int = 3) -> List[Dict]:
        """ä¸»è¦ç‰¹è¨±ã‚’æŠ½å‡º"""
        key_patents = []
        
        # ç‰¹è¨±æ¨©ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        patents = [ip for ip in industrial_properties if ip.get("industrial_property_right_type") == "patent_right"]
        
        for i, patent in enumerate(patents[:limit]):
            # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
            patent_title = patent.get("industrial_property_right_title", {})
            
            if isinstance(patent_title, str):
                title = patent_title
            elif isinstance(patent_title, dict):
                title = patent_title.get("ja", "")
                if not title:
                    title = patent_title.get("en", "")
            else:
                title = "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
            
            # å‡ºé¡˜ç•ªå·ãƒ»ç‰¹è¨±ç•ªå·
            application_number = patent.get("application_number", "")
            patent_number = patent.get("patent_number", "")
            
            # å‡ºé¡˜æ—¥
            application_date = patent.get("application_date", "")
            if isinstance(application_date, dict):
                year = application_date.get("year", "")
                month = application_date.get("month", "")
                day = application_date.get("day", "")
                application_date = f"{year}/{month}/{day}" if year else "ä¸æ˜"
            
            # ç‰¹è¨±æ¨©è€…ï¼ˆå‡ºé¡˜äººï¼‰ã®å–å¾—
            applicants = patent.get("applicants", [])
            applicant_names = []
            for applicant in applicants:
                if isinstance(applicant, dict):
                    applicant_info = applicant.get("applicant", {})
                    if isinstance(applicant_info, dict):
                        applicant_names.append(applicant_info.get("ja", applicant_info.get("en", "")))
                    elif isinstance(applicant_info, str):
                        applicant_names.append(applicant_info)
            
            # ç‰¹è¨±æ¨©è€…ãŒå–å¾—ã§ããªã„å ´åˆã¯ç™ºæ˜è€…ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not applicant_names:
                inventors = patent.get("inventors", {})
                if isinstance(inventors, dict) and "ja" in inventors:
                    inventor_list = inventors.get("ja", [])
                    for inventor in inventor_list:
                        if isinstance(inventor, dict):
                            name = inventor.get("name", {})
                            if isinstance(name, dict):
                                applicant_names.append(name.get("ja", name.get("en", "")))
                            elif isinstance(name, str):
                                applicant_names.append(name)
            
            key_patents.append({
                "title": title,
                "application_number": application_number,
                "patent_number": patent_number,
                "application_date": application_date,
                "patent_holders": "ã€".join(applicant_names) if applicant_names else "ç‰¹è¨±æ¨©è€…ä¸æ˜"
            })
        
        return key_patents
    
    def _extract_recent_presentations(self, presentations: List[Dict], limit: int = 3) -> List[Dict]:
        """æœ€è¿‘ã®ç™ºè¡¨ã‚’æŠ½å‡º"""
        recent_presentations = []
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        sorted_presentations = sorted(
            presentations,
            key=lambda x: x.get("publication_date", ""),
            reverse=True
        )
        
        for presentation in sorted_presentations[:limit]:
            title = presentation.get("presentation_title", {})
            title_text = title.get("ja", title.get("en", "N/A"))
            
            event = presentation.get("event", {})
            event_name = event.get("ja", event.get("en", "N/A"))
            
            date = presentation.get("publication_date", "N/A")
            
            recent_presentations.append({
                "title": title_text,
                "event": event_name,
                "date": date,
                "type": presentation.get("presentation_type", "N/A")
            })
        
        return recent_presentations
    
    async def _generate_detailed_analysis(
        self,
        profile: Dict,
        paper_count: int,
        misc_count: int,
        papers: List[Dict],
        projects: List[Dict],
        awards: List[Dict],
        query: str,
        research_interests: List[Dict] = None,
        presentations: List[Dict] = None,
        industrial_properties: List[Dict] = None
    ) -> str:
        """LLMã«ã‚ˆã‚‹è©³ç´°åˆ†æã®ç”Ÿæˆ"""
        
        research_interests = research_interests or []
        presentations = presentations or []
        industrial_properties = industrial_properties or []
        
        keywords = self._extract_research_keywords(research_interests, [])
        
        paper_titles_text = ""
        for i, paper in enumerate(papers[:5]):
            paper_title_data = paper.get("paper_title") or paper.get("published_paper_title", {})
            title = ""
            if isinstance(paper_title_data, dict):
                title = paper_title_data.get("ja", paper_title_data.get("en", ""))
            if title:
                paper_titles_text += f"- {title}\n"

        prompt = f"""
ã‚ãªãŸã¯ã€ä¼æ¥­ã®ç ”ç©¶é–‹ç™ºæ‹…å½“è€…å‘ã‘ã«ã€å¤§å­¦ç ”ç©¶è€…ã®å°‚é–€æ€§ã‚’è©•ä¾¡ã™ã‚‹ãƒ—ãƒ­ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ç ”ç©¶è€…æƒ…å ±ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’åŸºã«ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªè¦–ç‚¹ã‹ã‚‰è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### ç ”ç©¶è€…æƒ…å ±
- ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords) if keywords else 'N/A'}
- è«–æ–‡æ•°: {paper_count}ä»¶
- ãã®ä»–æ¥­ç¸¾æ•°: {misc_count}ä»¶
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {len(projects)}ä»¶
- å—è³æ­´: {len(awards)}ä»¶
- ç™ºè¡¨æ•°: {len(presentations)}ä»¶
- ç‰¹è¨±æ•°: {len(industrial_properties)}ä»¶
- ä¸»è¦è«–æ–‡ãƒ»æ¥­ç¸¾ï¼ˆæœ€æ–°5ä»¶ï¼‰:
{paper_titles_text if paper_titles_text else "N/A"}

### æ¤œç´¢ã‚¯ã‚¨ãƒª
ã€Œ{query}ã€

### åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®è¦ä»¶
ä»¥ä¸‹ã®ã€Œé«˜å“è³ªãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä¾‹ã€ã‚’å‚è€ƒã«ã€åŒæ§˜ã®æ§‹æˆã¨å“è³ªã§ã€ä»Šå›ã®ç ”ç©¶è€…ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ï¿½ï¿½ã„ã€‚
å„é …ç›®ã«ã¤ã„ã¦ã€å˜ã«æƒ…å ±ã‚’ç¾…åˆ—ã™ã‚‹ã®ã§ã¯ãªãã€æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£æ€§ã‚’æ·±ãè€ƒå¯Ÿã—ã€å…·ä½“çš„ãªè©•ä¾¡ã‚„è¦‹è§£ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ã€Œå®Ÿç¸¾ãƒ»å½±éŸ¿åŠ›ã€ã®é …ç›®ã§ã¯ã€è«–æ–‡æ•°ã¨ãã®ä»–æ¥­ç¸¾æ•°ã‚’åˆ†ã‘ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

---
### é«˜å“è³ªãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä¾‹
ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªï¼šã€ŒãƒŠãƒææ–™ã€äººå·¥çŸ¥èƒ½ã€å†ç”ŸåŒ»ç™‚ã€ã®å ´åˆï¼‰

1.  **æŠ€è¡“çš„é–¢é€£æ€§**: ã€ŒãƒŠãƒææ–™ã€ã¯ã€ã‚°ãƒ©ãƒ•ã‚§ãƒ³ã‚„ã‚«ãƒ¼ãƒœãƒ³ãƒŠãƒãƒãƒ¥ãƒ¼ãƒ–ãªã©ã®ææ–™ã‚’æŒ‡ã—ã€AIãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åŸºç›¤ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€Œäººå·¥çŸ¥èƒ½ã€ã¯ã€ãƒªã‚¶ãƒãƒ¼æ¼”ç®—å­ã‚„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ«ãƒ•ã‚£ãƒƒã‚¯AIãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®é–‹ç™ºãã®ã‚‚ã®ã‚’æŒ‡ã—ã¾ã™ã€‚ä¸€æ–¹ã€ã€Œå†ç”ŸåŒ»ç™‚ã€ã¨ã®ç›´æ¥çš„ãªé–¢é€£æ€§ã¯ã€ç¾æ™‚ç‚¹ã§ã¯è¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€‚ã—ã‹ã—ã€è„³å‹ãƒ‡ãƒã‚¤ã‚¹ã®é–‹ç™ºã¯ã€å°†æ¥çš„ã«è„³æ©Ÿèƒ½ã®è§£æ˜ã‚„ã€ç¥çµŒç´°èƒã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ç¹‹ãŒã‚Šã€å†ç”ŸåŒ»ç™‚åˆ†é‡ã¸ã®å¿œç”¨å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ã¨è¨€ãˆã¾ã™ã€‚
2.  **å®Ÿç¸¾ãƒ»å½±éŸ¿åŠ›**: è«–æ–‡æ•°158ä»¶ã€ãã®ä»–æ¥­ç¸¾25ä»¶ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°20ä»¶ã€ç™ºè¡¨æ•°20ä»¶ã¨ï¿½ï¿½ã†å®Ÿç¸¾ã¯ã€æ´»ç™ºãªç ”ç©¶æ´»å‹•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«è«–æ–‡æ•°ãŒå¤šãã€å­¦è¡“çš„ãªè²¢çŒ®åº¦ãŒé«˜ã„ã¨è©•ä¾¡ã§ãã¾ã™ã€‚å—è³æ­´2ä»¶ã¯ã€ç ”ç©¶æˆæœã®è³ªã‚’è£ä»˜ã‘ã‚‹æŒ‡æ¨™ã¨ãªã‚Šã¾ã™ã€‚ç‰¹è¨±å–å¾—ãŒãªã„ç‚¹ã¯ã€åŸºç¤ç ”ç©¶ã«é‡ç‚¹ã‚’ç½®ã„ã¦ã„ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯çŸ¥çš„è²¡ç”£æˆ¦ç•¥ãŒç•°ãªã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚
3.  **å®Ÿç”¨åŒ–å¯èƒ½æ€§**: ç”£å­¦é€£æºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿç¸¾ãŒã‚ã‚Šã€ä¼æ¥­ã¨ã®å…±åŒç ”ç©¶ã«ç©æ¥µçš„ã§ã‚ã‚‹ã¨æ¨æ¸¬ã•ã‚Œã¾ã™ã€‚ç‰¹ã«ã€AIãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ç ”ç©¶ã¯ã€åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚„ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒ¼ã‚«ãƒ¼ã¨ã®é€£æºãŒæœŸå¾…ã§ãã¾ã™ã€‚
---

### ã‚ãªãŸãŒä½œæˆã™ã‚‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªï¼šã€Œ{query}ã€ã®å ´åˆï¼‰

1.  **æŠ€è¡“çš„é–¢é€£æ€§**:
2.  **å®Ÿç¸¾ãƒ»å½±éŸ¿åŠ›**:
3.  **å®Ÿç”¨åŒ–å¯èƒ½æ€§**:

ä¸Šè¨˜3ã¤ã®é …ç›®ã«ã¤ã„ã¦ã€400å­—ã€œ500å­—ç¨‹åº¦ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            if "gemini" in self.model_name:
                response = self.llm_model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0.4,
                        "max_output_tokens": 800,
                        "top_p": 0.9
                    }
                )
                return response.text.strip()
            else:
                response = self.llm_model.predict(
                    prompt,
                    temperature=0.4,
                    max_output_tokens=800,
                    top_p=0.9
                )
                return response.text.strip()
                
        except Exception as e:
            logger.error(f"âŒ LLMåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_simple_analysis(
                self._get_name(profile), query, len(papers), len(projects)
            )
    
    async def _calculate_scores(
        self,
        profile: Dict,
        papers: List[Dict],
        projects: List[Dict],
        awards: List[Dict],
        query: str,
        research_interests: List[Dict] = None,
        research_areas: List[Dict] = None
    ) -> Dict[str, int]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + LLMï¼‰"""
        
        # 1. ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        rule_based_scores = self._calculate_rule_based_scores(
            papers, projects, awards, query, research_interests, research_areas
        )
        
        # 2. LLMãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯LLMãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ã‚‚è¨ˆç®—
        if self.llm_model:
            try:
                llm_scores = await self._calculate_llm_based_scores(
                    profile, papers, projects, awards, query, research_interests
                )
                
                # 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
                hybrid_scores = self._calculate_hybrid_scores(
                    rule_based_scores, llm_scores, weight=0.6  # LLMã®é‡ã¿ã‚’60%ã«
                )
                
                return hybrid_scores
                
            except Exception as e:
                logger.warning(f"LLMã‚¹ã‚³ã‚¢è¨ˆç®—å¤±æ•—ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ä½¿ç”¨: {e}")
                return rule_based_scores
        else:
            # LLMãŒä½¿ãˆãªã„å ´åˆã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿
            return rule_based_scores
    
    def _calculate_rule_based_scores(
        self,
        papers: List[Dict],
        projects: List[Dict],
        awards: List[Dict],
        query: str,
        research_interests: List[Dict] = None,
        research_areas: List[Dict] = None
    ) -> Dict[str, int]:
        """æ”¹è‰¯ç‰ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        technical_relevance = self._calculate_technical_relevance_improved(
            papers, projects, query, research_interests, research_areas
        )
        achievements = self._calculate_achievements(papers, projects, awards)
        practical_applicability = self._calculate_practical_applicability(projects)
        
        total = technical_relevance + achievements + practical_applicability
        
        return {
            "total": total,
            "technical_relevance": technical_relevance,
            "achievements": achievements,
            "practical_applicability": practical_applicability,
            "calculation_method": "rule_based"
        }
    
    def _extract_query_keywords(self, query: str) -> List[str]:
        """ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆåŒç¾©èªå±•é–‹å«ã‚€ï¼‰"""
        query_lower = query.lower()
        keywords = [query_lower]  # å…ƒã®ã‚¯ã‚¨ãƒª
        
        # ã€ŒãŒã‚“ã€ã€Œç™Œã€ã®åŒç¾©èªå‡¦ç†
        if "ãŒã‚“" in query_lower:
            keywords.append(query_lower.replace("ãŒã‚“", "ç™Œ"))
            keywords.append(query_lower.replace("ãŒã‚“", ""))  # ã€Œè…è‡“ãŒã‚“ã€â†’ã€Œè…è‡“ã€
            # éƒ¨åˆ†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            keywords.append("ãŒã‚“")
            keywords.append("ç™Œ")
        if "ç™Œ" in query_lower:
            keywords.append(query_lower.replace("ç™Œ", "ãŒã‚“"))
            keywords.append(query_lower.replace("ç™Œ", ""))
            keywords.append("ç™Œ")
            keywords.append("ãŒã‚“")
        
        # ã€Œè…è‡“ã€ã¨ã€Œè…ã€ã®åŒç¾©èªå‡¦ç†
        if "è…è‡“" in query_lower:
            keywords.append(query_lower.replace("è…è‡“", "è…"))
            keywords.append("è…è‡“")
            keywords.append("è…")
        elif "è…" in query_lower:
            keywords.append(query_lower.replace("è…", "è…è‡“"))
            keywords.append("è…")
            keywords.append("è…è‡“")
        
        # ã€Œæ²»ç™‚ã€é–¢é€£
        if "æ²»ç™‚" in query_lower:
            keywords.append("æ²»ç™‚")
            keywords.append("ç™‚æ³•")
            keywords.append("æ‰‹è¡“")
        
        # ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚è¿½åŠ 
        keywords.extend(query_lower.split())
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
        return list(set(keywords))
    
    def _calculate_technical_relevance_improved(
        self,
        papers: List[Dict],
        projects: List[Dict],
        query: str,
        research_interests: List[Dict] = None,
        research_areas: List[Dict] = None
    ) -> int:
        """æ”¹è‰¯ç‰ˆæŠ€è¡“çš„é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆæœ€å¤§40ç‚¹ï¼‰"""
        score = 0
        query_lower = query.lower()
        
        # ã‚¯ã‚¨ãƒªã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†è§£ï¼ˆåŒç¾©èªå±•é–‹å«ã‚€ï¼‰
        query_keywords = self._extract_query_keywords(query_lower)
        
        # 1. ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç ”ç©¶åˆ†é‡ã¨ã®é–¢é€£æ€§ï¼ˆæœ€å¤§15ç‚¹ï¼‰
        keyword_score = 0
        
        # research_interestsã‹ã‚‰
        if research_interests:
            for interest in research_interests:
                keyword_dict = interest.get("keyword", {})
                research_keyword = keyword_dict.get("ja", "").lower()
                
                # å®Œå…¨ä¸€è‡´
                if query_lower in research_keyword or research_keyword in query_lower:
                    keyword_score += 5
                # éƒ¨åˆ†ä¸€è‡´
                elif any(kw in research_keyword for kw in query_keywords):
                    keyword_score += 3
        
        # research_areasã‹ã‚‰
        if research_areas:
            for area in research_areas:
                research_field = area.get("research_field", {})
                field_name = research_field.get("ja", "").lower()
                
                research_keyword = area.get("research_keyword", {})
                if isinstance(research_keyword, dict):
                    keyword_text = research_keyword.get("ja", "").lower()
                elif isinstance(research_keyword, str):
                    keyword_text = research_keyword.lower()
                else:
                    keyword_text = ""
                
                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¾ãŸã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                for text in [field_name, keyword_text]:
                    if text:
                        if any(kw in text for kw in query_keywords):
                            keyword_score += 2
        
        score += min(15, keyword_score)
        
        # 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã®é–¢é€£æ€§ï¼ˆæœ€å¤§10ç‚¹ï¼‰
        project_score = 0
        for project in projects:
            project_title = project.get("research_project_title") or project.get("project_title", {})
            
            if isinstance(project_title, str):
                title = project_title.lower()
            elif isinstance(project_title, dict):
                title = project_title.get("ja", "").lower()
            else:
                title = ""
            
            # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
            if query_lower in title:
                project_score += 5
            elif any(kw in title for kw in query_keywords):
                project_score += 3
        
        score += min(10, project_score)
        
        # 3. è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã¨ã®é–¢é€£æ€§ï¼ˆæœ€å¤§10ç‚¹ï¼‰
        paper_score = 0
        for paper in papers:
            paper_title_data = paper.get("paper_title") or paper.get("published_paper_title", {})
            
            if isinstance(paper_title_data, dict):
                title_ja = paper_title_data.get("ja", "").lower()
                title_en = paper_title_data.get("en", "").lower()
            else:
                title_ja = ""
                title_en = ""
            
            # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
            if query_lower in title_ja or query_lower in title_en:
                paper_score += 2  # å®Œå…¨ä¸€è‡´
            elif any(kw in title_ja or kw in title_en for kw in query_keywords):
                paper_score += 1  # éƒ¨åˆ†ä¸€è‡´
        
        score += min(10, paper_score)
        
        # 4. ç ”ç©¶ã®æœ€æ–°æ€§ï¼ˆæœ€å¤§5ç‚¹ï¼‰
        recent_items = 0
        
        # æœ€è¿‘ã®è«–æ–‡
        for paper in papers[:5]:
            year = paper.get("publication_date", "")
            if isinstance(year, dict):
                year = year.get("year", 0)
            elif isinstance(year, str) and len(year) >= 4:
                year = int(year[:4]) if year[:4].isdigit() else 0
            else:
                year = 0
            
            if year >= 2020:
                recent_items += 1
        
        # æœ€è¿‘ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
        for project in projects[:3]:
            to_date = project.get("to_date", "")
            if not to_date or to_date == "ç¶™ç¶šä¸­":
                recent_items += 1
        
        score += min(5, recent_items)
        
        return min(40, score)  # æœ€å¤§40ç‚¹
    
    def _calculate_achievements(self, papers: List[Dict], projects: List[Dict], awards: List[Dict]) -> int:
        """å®Ÿç¸¾ãƒ»å½±éŸ¿åŠ›ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆæœ€å¤§30ç‚¹ï¼‰"""
        score = 0
        
        # è«–æ–‡æ•°ï¼ˆæœ€å¤§10ç‚¹ï¼‰
        if len(papers) >= 50:
            score += 10
        elif len(papers) >= 30:
            score += 8
        elif len(papers) >= 20:
            score += 6
        elif len(papers) >= 10:
            score += 4
        elif len(papers) >= 5:
            score += 2
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°ï¼ˆæœ€å¤§10ç‚¹ï¼‰
        if len(projects) >= 10:
            score += 10
        elif len(projects) >= 5:
            score += 7
        elif len(projects) >= 3:
            score += 5
        elif len(projects) >= 1:
            score += 3
        
        # å—è³æ­´ï¼ˆæœ€å¤§10ç‚¹ï¼‰
        score += min(10, len(awards) * 3)
        
        return score
    
    def _calculate_practical_applicability(self, projects: List[Dict]) -> int:
        """å®Ÿç”¨åŒ–å¯èƒ½æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆæœ€å¤§30ç‚¹ï¼‰"""
        score = 0
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°ãƒ™ãƒ¼ã‚¹
        if len(projects) >= 5:
            score += 15
        elif len(projects) >= 3:
            score += 10
        elif len(projects) >= 1:
            score += 5
        
        # ç”£å­¦é€£æºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¨å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ï¼‰
        industry_keywords = ["ä¼æ¥­", "ç”£å­¦", "å®Ÿç”¨", "é–‹ç™º", "è£½å“", "ã‚·ã‚¹ãƒ†ãƒ ", "å¿œç”¨"]
        industry_projects = 0
        
        for project in projects:
            # ResearchMap APIã®å®Ÿéš›ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«å¯¾å¿œ
            project_title = project.get("research_project_title") or project.get("project_title", {})
            
            if isinstance(project_title, str):
                title = project_title
            elif isinstance(project_title, dict):
                title = project_title.get("ja", "")
                if not title:
                    title = project_title.get("en", "")
            else:
                title = ""
            
            title_text = title.lower()
            if any(keyword in title_text for keyword in industry_keywords):
                industry_projects += 1
        
        score += min(15, industry_projects * 5)
        
        return score
    
    def _calculate_simple_scores(
        self,
        total_papers: int,
        total_projects: int,
        total_awards: int,
        query: str,
        papers: List[Dict]
    ) -> Dict[str, int]:
        """LLMã‚’ä½¿ã‚ãªã„ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # ç°¡æ˜“çš„ãªæŠ€è¡“çš„é–¢é€£æ€§ï¼ˆè«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰
        technical_relevance = min(40, total_papers)
        
        # å®Ÿç¸¾ã‚¹ã‚³ã‚¢
        achievements = min(30, (total_papers // 3) + (total_projects * 2) + (total_awards * 3))
        
        # å®Ÿç”¨åŒ–å¯èƒ½æ€§
        practical_applicability = min(30, total_projects * 5)
        
        return {
            "total": technical_relevance + achievements + practical_applicability,
            "technical_relevance": technical_relevance,
            "achievements": achievements,
            "practical_applicability": practical_applicability
        }
    
    def _generate_simple_analysis(
        self,
        researcher_name: str,
        query: str,
        total_papers: int,
        total_projects: int
    ) -> str:
        """LLMã‚’ä½¿ã‚ãªã„ç°¡æ˜“åˆ†æ"""
        return f"""ã“ã®ç ”ç©¶è€…ã¯ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ãŠã‚Šã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

1. ç ”ç©¶å®Ÿç¸¾ï¼š{total_papers}ä»¶ã®è«–æ–‡ã‚’ç™ºè¡¨ã—ã¦ãŠã‚Šã€æ´»ç™ºãªç ”ç©¶æ´»å‹•ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿç¸¾ï¼š{total_projects}ä»¶ã®ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å‚ç”»ã—ã¦ãŠã‚Šã€å®Ÿè·µçš„ãªç ”ç©¶çµŒé¨“ã‚’æœ‰ã—ã¦ã„ã¾ã™ã€‚

3. å°‚é–€æ€§ï¼š{query}åˆ†é‡ã§ã®ç ”ç©¶æ´»å‹•ãŒç¢ºèªã§ãã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è²¢çŒ®ãŒæœŸå¾…ã§ãã¾ã™ã€‚

ç·åˆçš„ã«ã€ã“ã®ç ”ç©¶è€…ã¯{query}ã®åˆ†é‡ã§ä¸€å®šã®å°‚é–€æ€§ã¨å®Ÿç¸¾ã‚’æŒã£ã¦ãŠã‚Šã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®å‚ç”»ãŒæœ‰ç›Šã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚"""
    
    async def _calculate_llm_based_scores(
        self,
        profile: Dict,
        papers: List[Dict],
        projects: List[Dict],
        awards: List[Dict],
        query: str,
        research_interests: List[Dict] = None
    ) -> Dict[str, int]:
        """LLMã‚’ä½¿ç”¨ã—ãŸã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keywords = self._extract_research_keywords(research_interests or [], [])
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆä½œæˆ
        project_titles = []
        for project in projects[:5]:  # æœ€æ–°5ä»¶
            project_title = project.get("research_project_title") or project.get("project_title", {})
            if isinstance(project_title, str):
                project_titles.append(project_title)
            elif isinstance(project_title, dict):
                title = project_title.get("ja", project_title.get("en", ""))
                if title:
                    project_titles.append(title)
        
        # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®ç ”ç©¶è€…æƒ…å ±ã‚’åŸºã«ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã¨ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€ç ”ç©¶è€…æƒ…å ±ã€‘
- ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {''.join(keywords[:10]) if keywords else 'ãªã—'}
- è«–æ–‡æ•°: {len(papers)}ä»¶
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_titles}
- å—è³æ­´: {len(awards)}ä»¶

ã€è©•ä¾¡åŸºæº–ã€‘
1. æŠ€è¡“çš„é–¢é€£æ€§ï¼ˆ0-40ç‚¹ï¼‰
   - ç ”ç©¶å†…å®¹ã¨ã‚¯ã‚¨ãƒªã®ç›´æ¥çš„ãªé–¢é€£æ€§
   - ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´åº¦
   - é–¢é€£è«–æ–‡ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ‰ç„¡
   - åŒç¾©èªã‚„é¡ä¼¼æ¦‚å¿µã‚‚è€ƒæ…®ï¼ˆä¾‹ï¼šã€Œç™Œã€ã¨ã€ŒãŒã‚“ã€ã€ã€Œè…ã€ã¨ã€Œè…è‡“ã€ï¼‰

2. å®Ÿç¸¾ãƒ»å½±éŸ¿åŠ›ï¼ˆ0-30ç‚¹ï¼‰
   - è«–æ–‡æ•°ã¨è³ª
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿç¸¾
   - å—è³æ­´

3. å®Ÿç”¨åŒ–å¯èƒ½æ€§ï¼ˆ0-30ç‚¹ï¼‰
   - ç”£å­¦é€£æºã®å®Ÿç¸¾
   - å®Ÿç”¨çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ‰ç„¡
   - æ²»ç™‚æ³•ã‚„æŠ€è¡“ã®å®Ÿè£…å¯èƒ½æ€§

ã€é‡è¦ã€‘
- å„é …ç›®ã®ç‚¹æ•°ã¯å¿…ãšæŒ‡å®šã•ã‚ŒãŸç¯„å›²å†…ã§æ¡ç‚¹ã—ã¦ãã ã•ã„
- éƒ¨åˆ†çš„ãªé–¢é€£æ€§ã‚‚é©åˆ‡ã«è©•ä¾¡ã—ã¦ãã ã•ã„
- ã€Œè…ç™Œã€ã¨ã€Œè…è‡“ãŒã‚“ã€ã®ã‚ˆã†ãªåŒç¾©èªã¯åŒç­‰ã«æ‰±ã£ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "technical_relevance": æ•°å€¤ï¼ˆ0-40ï¼‰,
    "achievements": æ•°å€¤ï¼ˆ0-30ï¼‰,
    "practical_applicability": æ•°å€¤ï¼ˆ0-30ï¼‰,
    "reasoning": {{
        "technical_relevance": "æ¡ç‚¹ç†ç”±",
        "achievements": "æ¡ç‚¹ç†ç”±",
        "practical_applicability": "æ¡ç‚¹ç†ç”±"
    }}
}}"""
        
        try:
            if "gemini" in self.model_name:
                response = self.llm_model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0,  # ä¸€è²«æ€§ã®ãŸã‚
                        "max_output_tokens": 400,
                        "top_p": 0.8
                    }
                )
                response_text = response.text.strip()
            else:
                response = self.llm_model.predict(
                    prompt,
                    temperature=0,
                    max_output_tokens=400,
                    top_p=0.8
                )
                response_text = response.text.strip()
            
            # JSONã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾å¿œï¼‰
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            # JSONãƒ‘ãƒ¼ã‚¹
            scores = json.loads(response_text)
            
            # ã‚¹ã‚³ã‚¢ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ã¨å‹å¤‰æ›
            scores["technical_relevance"] = int(min(40, max(0, scores.get("technical_relevance", 0))))
            scores["achievements"] = int(min(30, max(0, scores.get("achievements", 0))))
            scores["practical_applicability"] = int(min(30, max(0, scores.get("practical_applicability", 0))))
            scores["total"] = scores["technical_relevance"] + scores["achievements"] + scores["practical_applicability"]
            scores["calculation_method"] = "llm_based"
            
            # reasoningæƒ…å ±ã‚‚ä¿æŒ
            if "reasoning" in scores:
                scores["score_reasons"] = scores.pop("reasoning")
            
            logger.info(f"LLMã‚¹ã‚³ã‚¢è¨ˆç®—æˆåŠŸ: {scores}")
            return scores
            
        except Exception as e:
            logger.error(f"LLMã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            raise e
    
    def _calculate_hybrid_scores(
        self,
        rule_based_scores: Dict[str, int],
        llm_scores: Dict[str, int],
        weight: float = 0.5
    ) -> Dict[str, int]:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã¨LLMã‚¹ã‚³ã‚¢ã®åŠ é‡å¹³å‡"""
        
        hybrid_scores = {}
        
        # å„ã‚¹ã‚³ã‚¢ã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—
        for key in ["technical_relevance", "achievements", "practical_applicability"]:
            rule_score = rule_based_scores.get(key, 0)
            llm_score = llm_scores.get(key, 0)
            
            # åŠ é‡å¹³å‡ï¼ˆå°æ•°ç‚¹ä»¥ä¸‹ã¯å››æ¨äº”å…¥ï¼‰
            hybrid_scores[key] = int(round(
                rule_score * (1 - weight) + llm_score * weight
            ))
        
        # åˆè¨ˆã‚¹ã‚³ã‚¢
        hybrid_scores["total"] = sum([
            hybrid_scores["technical_relevance"],
            hybrid_scores["achievements"],
            hybrid_scores["practical_applicability"]
        ])
        
        # ãƒ¡ã‚¿æƒ…å ±
        hybrid_scores["calculation_method"] = "hybrid"
        hybrid_scores["rule_weight"] = 1 - weight
        hybrid_scores["llm_weight"] = weight
        
        # å„æ‰‹æ³•ã®ã‚¹ã‚³ã‚¢ã‚‚ä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        hybrid_scores["rule_based_scores"] = rule_based_scores
        hybrid_scores["llm_scores"] = llm_scores
        
        # LLMã®æ¡ç‚¹ç†ç”±ãŒã‚ã‚Œã°å«ã‚ã‚‹
        if "score_reasons" in llm_scores:
            hybrid_scores["score_reasons"] = llm_scores["score_reasons"]
        
        logger.info(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢è¨ˆç®—å®Œäº†: {hybrid_scores}")
        return hybrid_scores
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä½œæˆ"""
        return {
            "status": "error",
            "error": error_message,
            "analysis": None
        }
    
    def _create_mock_researcher_data(self, researcher_id: str) -> Dict[str, Any]:
        """ãƒ¢ãƒƒã‚¯ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆAPIåˆ©ç”¨ä¸å¯æ™‚ï¼‰"""
        return {
            "profile": {
                "user_id": researcher_id,
                "permalink": f"https://researchmap.jp/{researcher_id}",
                "display_name": {"ja": "ã‚µãƒ³ãƒ—ãƒ«ç ”ç©¶è€…"},
                "affiliations": [{
                    "affiliation": {"ja": "ã‚µãƒ³ãƒ—ãƒ«å¤§å­¦"}
                }]
            },
            "papers": [
                {
                    "paper_title": {"ja": "ã‚µãƒ³ãƒ—ãƒ«è«–æ–‡1ï¼šAIææ–™æ¢ç´¢ã®æ‰‹æ³•ææ¡ˆ"},
                    "publication_date": "2023"
                },
                {
                    "paper_title": {"ja": "ã‚µãƒ³ãƒ—ãƒ«è«–æ–‡2ï¼šãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–"},
                    "publication_date": "2024"
                }
            ],
            "projects": [
                {
                    "research_project_title": {"ja": "ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼šäººå·¥çŸ¥èƒ½ã‚’ç”¨ã„ãŸæ–°æŠ€è¡“ã®é–‹ç™º"},
                    "from_date": "2023",
                    "to_date": "ç¶™ç¶šä¸­"
                }
            ],
            "awards": [
                {
                    "award_title": "å„ªç§€ç ”ç©¶è³",
                    "award_date": "2023"
                }
            ],
            "research_interests": [
                {
                    "keyword": {"ja": "äººå·¥çŸ¥èƒ½"}
                },
                {
                    "keyword": {"ja": "æ©Ÿæ¢°å­¦ç¿’"}
                },
                {
                    "keyword": {"ja": "ãƒãƒ†ãƒªã‚¢ãƒ«ã‚ºã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹"}
                }
            ],
            "research_areas": [],
            "presentations": [
                {
                    "presentation_title": {"ja": "ã‚µãƒ³ãƒ—ãƒ«ç™ºè¡¨ï¼šAIç ”ç©¶æˆæœã®å ±å‘Š"},
                    "event": {"ja": "å­¦è¡“ä¼šè­°"},
                    "publication_date": "2024"
                }
            ],
            "misc_publications": [],
            "industrial_properties": []
        }
