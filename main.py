"""
ç ”ç©¶è€…æ¤œç´¢API - v2.1.1 (æœ€çµ‚ä¿®æ­£ç‰ˆ)
"""

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel, Field
import pandas as pd
import os
import time
import asyncio
from typing import List, Optional, Dict, Any
import logging

from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# project_managerã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from project_manager import (
    project_manager,
    ProjectCreateRequest,
    ResearcherSelectionRequest,
    MatchingRequest,
    TempProject
)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = FastAPI(
    title="ç ”ç©¶è€…æ¤œç´¢API",
    description="AIç ”ç©¶è€…æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ",
    version="2.1.1"
)
# ä½™è¨ˆãªãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’é˜²ã
app.router.redirect_slashes = False

# æœ¬ç•ª/ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã® Vercel ã‚’è¨±å¯
ALLOWED_ORIGIN_REGEX = r"^https:\/\/research-partner-dashboard(?:-[a-z0-9-]+)?\.vercel\.app$"

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    # allow_origins=["*"],
    allow_origin_regex=ALLOWED_ORIGIN_REGEX,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.getenv("PROJECT_ID", "apt-rope-217206")
LOCATION = os.getenv("LOCATION", "us-central1")
BIGQUERY_TABLE = os.getenv("BIGQUERY_TABLE", "apt-rope-217206.researcher_data.rd_250524")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä¿æŒ
clients = {
    "initialized": False,
    "bq_client": None,
    "main_llm_model": None,
    "summary_llm_model": None,
    "embedding_model": None
}

class SearchRequest(BaseModel):
    query: str
    method: str = "semantic"
    max_results: int = Field(5, alias='maxResults')
    exclude_keywords: Optional[List[str]] = Field(None, alias='excludeKeywords')
    use_llm_expansion: bool = Field(False, alias='useLlmExpansion')
    use_llm_summary: bool = Field(False, alias='useLlmSummary')
    use_internal_evaluation: bool = Field(False, alias='useInternalEvaluation')
    young_researcher_filter: bool = Field(False, alias='youngResearcherFilter')
    university_filter: Optional[List[str]] = Field(None, alias='universityFilter')

    class Config:
        allow_population_by_field_name = True

class ResearcherResult(BaseModel):
    name_ja: Optional[str] = None
    name_en: Optional[str] = None
    main_affiliation_name_ja: Optional[str] = None
    main_affiliation_name_en: Optional[str] = None
    main_affiliation_job_ja: Optional[str] = None
    main_affiliation_job_title_ja: Optional[str] = None
    main_affiliation_job_en: Optional[str] = None
    main_affiliation_job_title_en: Optional[str] = None
    research_keywords_ja: Optional[str] = None
    research_fields_ja: Optional[str] = None
    profile_ja: Optional[str] = None
    paper_title_ja_first: Optional[str] = None
    project_title_ja_first: Optional[str] = None
    researchmap_url: Optional[str] = None
    relevance_score: Optional[float] = None
    distance: Optional[float] = None
    llm_summary: Optional[str] = None
    is_young_researcher: Optional[bool] = None
    young_researcher_reasons: Optional[List[str]] = None

class SearchResponse(BaseModel):
    status: str
    query: str
    method: str
    results: List[ResearcherResult] = []
    total: int
    execution_time: float
    executed_query_info: Optional[str] = None
    expanded_info: Optional[dict] = None

class AnalyzeRequest(BaseModel):
    researchmap_url: str
    query: str
    include_keyword_map: bool = False
    researcher_basic_info: Optional[Dict[str, Any]] = None

class AnalysisResponse(BaseModel):
    status: str
    analysis: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# --- ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã•ã‚ŒãŸPydanticãƒ¢ãƒ‡ãƒ« ---

class ResearcherInfoPayload(BaseModel):
    """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰é€ä¿¡ã•ã‚Œã‚‹ç ”ç©¶è€…æƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«"""
    name_ja: Optional[str] = Field(None, alias='name_ja')
    research_fields_ja: Optional[str] = Field(None, alias='research_fields_ja')
    project_title_ja_first: Optional[str] = Field(None, alias='project_title_ja_first')
    paper_title_ja_first: Optional[str] = Field(None, alias='paper_title_ja_first')
    research_keywords_ja: Optional[str] = Field(None, alias='research_keywords_ja')
    profile_ja: Optional[str] = Field(None, alias='profile_ja')
    main_affiliation_name_ja: Optional[str] = Field(None, alias='main_affiliation_name_ja')

    class Config:
        allow_population_by_field_name = True

class SummaryRequest(BaseModel):
    """AIè¦ç´„ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ¢ãƒ‡ãƒ«"""
    researchmap_url: str
    query: str
    researcher_info: Optional[ResearcherInfoPayload] = None

# --- ã“ã“ã¾ã§ãŒä¿®æ­£ã•ã‚ŒãŸPydanticãƒ¢ãƒ‡ãƒ« ---

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
    logger.info("ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ - GCPåˆæœŸåŒ–ã‚’å®Ÿè¡Œ")
    try:
        from gcp_auth import initialize_gcp_on_startup
        success = initialize_gcp_on_startup()
        if success:
            logger.info("âœ… GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
            clients["initialized"] = True
        else:
            logger.warning("âš ï¸ GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•— - ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
            clients["initialized"] = False
    except Exception as e:
        logger.error(f"âŒ GCPåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        clients["initialized"] = False

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "ğŸš€ ç ”ç©¶è€…æ¤œç´¢API v2.1.1 ã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†çµ±åˆãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ç‰ˆï¼‰",
        "status": "healthy",
        "timestamp": time.time(),
        "version": "2.1.1",
        "endpoints": {
            "/health": "ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
            "/api/universities": "å¤§å­¦ãƒªã‚¹ãƒˆ",
            "/api/search": "ç ”ç©¶è€…æ¤œç´¢",
            "/api/analyze-researcher": "ç ”ç©¶è€…è©³ç´°åˆ†æ",
            "/api/temp-projects": "ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†",
            "/test_api.html": "ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«"
        },
        "features": {
            "search_api": "âœ… åˆ©ç”¨å¯èƒ½" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­",
            "gcp_integration": "âœ… æº–å‚™å®Œäº†" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­",
            "researchmap_analysis": "âœ… åˆ©ç”¨å¯èƒ½",
            "project_management": "âœ… åˆ©ç”¨å¯èƒ½",
            "matching_system": "âœ… åˆ©ç”¨å¯èƒ½"
        }
    }

@app.get("/test_api.html")
async def test_api_page():
    """ãƒ†ã‚¹ãƒˆAPIãƒšãƒ¼ã‚¸"""
    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é…ä¿¡ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
    # from fastapi.staticfiles import StaticFiles
    # app.mount("/static", StaticFiles(directory="static"), name="static")
    # ã®ã‚ˆã†ãªè¨­å®šãŒåˆ¥é€”å¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    # ã“ã“ã§ã¯FileResponseã§ç›´æ¥è¿”ã—ã¾ã™ã€‚
    if os.path.exists("test_api.html"):
        return FileResponse("test_api.html")
    raise HTTPException(status_code=404, detail="test_api.html not found")


@app.get("/health")
async def health_check():
    """è©³ç´°ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        from gcp_auth import get_gcp_status
        gcp_status = get_gcp_status()
    except Exception as e:
        gcp_status = {"error": str(e)}
    
    health_status = {
        "status": "healthy",
        "timestamp": time.time(),
        "server_info": {
            "version": "2.1.1",
            "project_id": PROJECT_ID,
            "location": LOCATION
        },
        "clients_status": {
            "initialized": clients["initialized"],
            "bigquery": "âœ… æº–å‚™å®Œäº†" if gcp_status.get("bigquery_ready") else "ğŸ”„ æº–å‚™ä¸­",
            "vertex_ai": "âœ… æº–å‚™å®Œäº†" if gcp_status.get("vertex_ai_ready") else "ğŸ”„ æº–å‚™ä¸­",
            "credentials": "âœ… è¨­å®šæ¸ˆ" if gcp_status.get("credentials_available") else "âŒ æœªè¨­å®š"
        },
        "endpoints": {
            "/": "âœ… åˆ©ç”¨å¯èƒ½",
            "/health": "âœ… åˆ©ç”¨å¯èƒ½",
            "/api/search": "âœ… å®Ÿéš›æ¤œç´¢å¯èƒ½" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­ï¼ˆãƒ¢ãƒƒã‚¯å¿œç­”ã‚ã‚Šï¼‰",
            "/api/universities": "âœ… åˆ©ç”¨å¯èƒ½",
            "/api/analyze-researcher": "âœ… ResearchMapåˆ†æå¯èƒ½",
            "/api/temp-projects": "âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†å¯èƒ½",
            "/test_api.html": "âœ… åˆ©ç”¨å¯èƒ½"
        },
        "gcp_details": gcp_status
    }
    return health_status


def get_simple_university_query(table_name: str) -> str:
    """
    ã€æœ€çµ‚æ”¹å–„ç‰ˆã€‘ç‰¹æ®Šãªçµ±åˆãƒ«ãƒ¼ãƒ«ã¨ã€ä¸€èˆ¬çš„ãªæ­£è¦åŒ–ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¯ã‚¨ãƒª
    """
    return f"""
    WITH base_data AS (
      SELECT 
        main_affiliation_name_ja,
        name_ja
      FROM `{table_name}`
      WHERE main_affiliation_name_ja IS NOT NULL AND main_affiliation_name_ja LIKE '%å¤§å­¦%'
    ),
    
    cleaned_names AS (
      SELECT
        CASE
          WHEN main_affiliation_name_ja LIKE '%å¥ˆè‰¯å…ˆç«¯ç§‘å­¦æŠ€è¡“å¤§å­¦é™¢å¤§å­¦%' THEN 'å¥ˆè‰¯å…ˆç«¯ç§‘å­¦æŠ€è¡“å¤§å­¦é™¢å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±äº¬å·¥æ¥­å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±æµ·å›½ç«‹å¤§å­¦%' THEN 'åå¤å±‹å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹%' THEN 'åå¤å±‹å¤§å­¦'
          ELSE
            TRIM(
                REGEXP_REPLACE(
                    REGEXP_REPLACE(
                        REGEXP_REPLACE(
                            main_affiliation_name_ja,
                            '^(å›½ç«‹å¤§å­¦æ³•äºº|å­¦æ ¡æ³•äºº|å…¬ç«‹å¤§å­¦æ³•äºº)\\\\s*', ''
                        ),
                        'ï¼.*$', ''
                    ),
                    '\\\\s*(å¤§å­¦é™¢|å¤§å­¦ç—…é™¢|ç—…é™¢|ç ”ç©¶é™¢|ç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼|ç ”ç©¶ç§‘|å­¦éƒ¨|é™„å±|ç‰¹ä»»å‡†æ•™æˆ|æ•™æˆ|å‡†æ•™æˆ|å®¢å“¡|æ©Ÿæ§‹|ã‚»ãƒ³ã‚¿ãƒ¼).*$', ''
                )
            )
        END AS university_name,
        name_ja,
        main_affiliation_name_ja as original_name
      FROM base_data
    ),

    validated_universities AS (
      SELECT 
        university_name,
        name_ja,
        original_name
      FROM cleaned_names
      WHERE university_name IS NOT NULL
        AND university_name LIKE '%å¤§å­¦'
        AND LENGTH(university_name) >= 3
    )
    
    SELECT 
      university_name,
      COUNT(DISTINCT name_ja) as researcher_count,
      ARRAY_AGG(DISTINCT original_name ORDER BY original_name LIMIT 5) as original_names
    FROM validated_universities
    GROUP BY university_name
    HAVING COUNT(DISTINCT name_ja) >= 5
    ORDER BY researcher_count DESC
    LIMIT 100
    """

@app.get("/api/universities")
async def get_universities():
    """
    ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å¤§å­¦åã¨ãã®ç ”ç©¶è€…æ•°ã‚’å–å¾—
    ã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆ
    """
    start_time = time.time()
    
    try:
        logger.info("ğŸ« å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—é–‹å§‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆï¼‰")
        
        try:
            from gcp_auth import get_bigquery_client, get_gcp_status
            logger.info("âœ… ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨")
        except ImportError as e:
            logger.error(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return await get_universities_fallback("module_import_error", str(e))
        
        gcp_status = get_gcp_status()
        logger.info(f"ğŸ“Š GCPçŠ¶æ³: {gcp_status}")
        
        bq_client = get_bigquery_client()
        
        if not bq_client:
            logger.warning("âš ï¸ BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰")
            return await get_universities_fallback("bigquery_unavailable", "BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        try:
            query = get_simple_university_query(BIGQUERY_TABLE)
            logger.info(f"âœ… ã‚·ãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªç”ŸæˆæˆåŠŸ: {len(query)}æ–‡å­—")
            
            logger.info("ğŸ” BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œé–‹å§‹")
            query_job = bq_client.query(query)
            
            universities = []
            normalization_details = []
            row_count = 0
            
            logger.info("â³ ã‚¯ã‚¨ãƒªçµæœã®å‡¦ç†ä¸­...")
            
            for row in query_job:
                row_count += 1
                
                if not row.university_name or "å¤§å­¦å¤§å­¦" in row.university_name:
                    if row.university_name:
                        logger.warning(f"âš ï¸ ç•°å¸¸ãªå¤§å­¦åã‚’ã‚¹ã‚­ãƒƒãƒ—: {row.university_name}")
                    continue
                
                if not row.university_name.endswith('å¤§å­¦'):
                    logger.warning(f"âš ï¸ ä¸æ­£ãªå¤§å­¦åã‚’ã‚¹ã‚­ãƒƒãƒ—: {row.university_name}")
                    continue
                
                university_data = {
                    "name": row.university_name,
                    "count": row.researcher_count
                }
                
                if hasattr(row, 'merge_info') and row.merge_info:
                    university_data["merge_info"] = row.merge_info
                    university_data["is_merged"] = True
                else:
                    university_data["is_merged"] = False
                
                if hasattr(row, 'original_names') and row.original_names:
                    university_data["original_names"] = row.original_names
                    if len(row.original_names) > 1:
                        normalization_details.append({
                            "normalized_name": row.university_name,
                            "original_names": row.original_names,
                            "consolidated_count": row.researcher_count,
                            "merge_info": getattr(row, 'merge_info', None)
                        })
                
                universities.append(university_data)
                
                if len(universities) <= 10:
                    merge_info = ""
                    if hasattr(row, 'merge_info') and row.merge_info:
                        merge_info = f" ğŸ”—çµ±åˆ: {row.merge_info}"
                    elif hasattr(row, 'original_names') and row.original_names and len(row.original_names) > 1:
                        merge_info = f" (çµ±åˆ: {len(row.original_names)}æ ¡)"
                    logger.info(f"  {len(universities)}. {row.university_name}: {row.researcher_count:,}å{merge_info}")
            
            execution_time = time.time() - start_time
            
            tokyo_kagaku = next((u for u in universities if u["name"] == "æ±äº¬ç§‘å­¦å¤§å­¦"), None)
            
            response = {
                "status": "success",
                "total_universities": len(universities),
                "universities": universities,
                "normalization_info": {
                    "method": "complete_university_integration_v4_safe",
                    "rules": [
                        "ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦",
                        "ğŸŒ æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹çµ±åˆ: æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹(åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                        "ğŸ›ï¸ å›½ç«‹å¤§å­¦æ³•äººã®é™¤å»ã¨çµ±åˆå‡¦ç†",
                        "ğŸ§¹ é™„å±æ©Ÿé–¢é™¤å¤–: å¤§å­¦é™¢ãƒ»ç—…é™¢ãƒ»ç ”ç©¶ç§‘ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼ç­‰ã‚’è¦ªå¤§å­¦ã«çµ±åˆ",
                        "âœ‚ï¸ ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–: é‡è¤‡ãƒ»ç©ºæ–‡å­—ãƒ»çŸ­ã™ãã‚‹åå‰",
                        "ğŸ“ é•·ã•åˆ¶é™: 3-15æ–‡å­—ã®é©åˆ‡ãªå¤§å­¦åã®ã¿",
                        "ğŸ”’ BigQueryå®‰å…¨ç‰ˆ: æ­£è¦è¡¨ç¾ã®å•é¡Œã‚’å›é¿ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"
                    ],
                    "consolidated_universities": len(normalization_details),
                    "details": normalization_details[:10],
                    "tokyo_kagaku_integration": {
                        "success": tokyo_kagaku is not None,
                        "count": tokyo_kagaku["count"] if tokyo_kagaku else 0,
                        "merge_info": tokyo_kagaku.get("merge_info") if tokyo_kagaku else None,
                        "expected_sources": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦"
                    },
                    "tokai_national_integration": {
                        "rule": "æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ (åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                        "reason": "åå¤å±‹å¤§å­¦ãŒä¸»è¦æ§‹æˆå¤§å­¦ã®ãŸã‚"
                    }
                },
                "execution_time": execution_time,
                "query_stats": {
                    "rows_processed": row_count,
                    "valid_universities": len(universities),
                    "merged_universities": len([u for u in universities if u.get("is_merged")]),
                    "query_length": len(query)
                },
                "debug_info": {
                    "bigquery_client_type": str(type(bq_client)),
                    "table_name": BIGQUERY_TABLE,
                    "gcp_status": gcp_status
                }
            }
            
            merged_count = len([u for u in universities if u.get("is_merged")])
            total_integration_count = len(normalization_details)
            
            if tokyo_kagaku:
                logger.info(f"ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆæˆåŠŸ: {tokyo_kagaku['count']:,}å")
            
            logger.info(f"âœ… å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—å®Œäº†: {len(universities)}æ ¡ (ç‰¹åˆ¥çµ±åˆ: {merged_count}æ ¡, ä¸€èˆ¬çµ±åˆ: {total_integration_count}æ ¡) {execution_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"âŒ BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")
            if 'query' in locals():
                logger.error(f"ğŸ” ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚¯ã‚¨ãƒª: {query}")
            return await get_universities_fallback("bigquery_execution_error", str(e))
            
    except Exception as e:
        logger.error(f"âŒ å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")
        return await get_universities_fallback("unexpected_error", str(e))

async def get_universities_fallback(error_type: str, error_message: str):
    """
    å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
    """
    logger.warning(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ: {error_type}")
    
    mock_universities = [
        {"name": "äº¬éƒ½å¤§å­¦", "count": 6264, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±äº¬å¤§å­¦", "count": 5113, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "å¤§é˜ªå¤§å­¦", "count": 4542, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±äº¬ç§‘å­¦å¤§å­¦", "count": 3503, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆçµ±åˆå¾Œï¼‰", "is_merged": True, "merge_info": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦"},
        {"name": "åŒ—æµ·é“å¤§å­¦", "count": 3515, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±åŒ—å¤§å­¦", "count": 3426, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "ä¹å·å¤§å­¦", "count": 2486, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "ç­‘æ³¢å¤§å­¦", "count": 2471, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "åå¤å±‹å¤§å­¦", "count": 2317, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False}
    ]
    
    return {
        "status": "fallback",
        "total_universities": len(mock_universities),
        "universities": mock_universities,
        "fallback_info": {
            "reason": error_type,
            "error_message": error_message,
            "note": "ã“ã‚Œã¯å®Œå…¨çµ±åˆç‰ˆã®æœŸå¾…çµæœã§ã™ã€‚æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆãŒæ­£ã—ãå‹•ä½œã—ã€4ä½ã«ãƒ©ãƒ³ã‚¯ã‚¤ãƒ³ã—ã¾ã™ã€‚"
        },
        "normalization_info": {
            "method": "complete_university_integration_v4",
            "rules": [
                "ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦",
                "ğŸŒ æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹çµ±åˆ: æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹(åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                "ğŸ›ï¸ å›½ç«‹å¤§å­¦æ³•äººã®é™¤å»ã¨çµ±åˆå‡¦ç†",
                "ğŸ§¹ é™„å±æ©Ÿé–¢é™¤å¤–: å¤§å­¦é™¢ãƒ»ç—…é™¢ãƒ»ç ”ç©¶ç§‘ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼ç­‰ã‚’è¦ªå¤§å­¦ã«çµ±åˆ",
                "âœ‚ï¸ ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–: é‡è¤‡ãƒ»ç©ºæ–‡å­—ãƒ»çŸ­ã™ãã‚‹åå‰",
                "ğŸ” è² ã®å…ˆèª­ã¿æ­£è¦è¡¨ç¾ã§ç¢ºå®Ÿãªè¦ªå¤§å­¦åæŠ½å‡º"
            ],
            "consolidated_universities": 25,
            "tokyo_kagaku_integration": { "success": True, "count": 3503, "sources": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦" },
            "tokai_national_integration": { "rule": "æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ (åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦", "reason": "åå¤å±‹å¤§å­¦ãŒä¸»è¦æ§‹æˆå¤§å­¦ã®ãŸã‚" },
            "note": "å®Œå…¨çµ±åˆå¯¾å¿œã®å¤§å­¦åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ "
        }
    }

def get_researcher_data_by_url(url: str) -> Optional[Dict[str, Any]]:
    """researchmap_urlã‚’ã‚­ãƒ¼ã«BigQueryã‹ã‚‰ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"""
    from gcp_auth import get_bigquery_client
    bq_client = get_bigquery_client()
    if not bq_client:
        logger.error("BigQuery client not available for summary generation.")
        return None

    query = f"SELECT * FROM `{BIGQUERY_TABLE}` WHERE researchmap_url = @url LIMIT 1"
    
    from google.cloud.bigquery import QueryJobConfig, ScalarQueryParameter
    job_config = QueryJobConfig(
        query_parameters=[ScalarQueryParameter("url", "STRING", url)]
    )
    
    try:
        logger.info(f"Querying BigQuery for researcher with URL: {url}")
        query_job = bq_client.query(query, job_config=job_config)
        results = query_job.to_dataframe()
        if results.empty:
            logger.warning(f"No researcher data found for URL: {url}")
            return None
        
        researcher_dict = results.iloc[0].where(pd.notnull(results.iloc[0]), None).to_dict()
        return researcher_dict
    except Exception as e:
        logger.error(f"BigQueryã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {e}")
        return None

# --- ã“ã“ã‹ã‚‰ãŒå¾©å…ƒã•ã‚ŒãŸæ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.post("/api/generate-summary")
async def generate_single_summary(request: SummaryRequest):
    """
    AIè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ç ”ç©¶è€…æƒ…å ±ãŒæä¾›ã•ã‚ŒãŸå ´åˆã¯ã€DBã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    """
    logger.info(f"ğŸ¤– AIè¦ç´„ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: {request.researchmap_url} (Query: {request.query})")
    
    researcher_data = None
    
    if request.researcher_info:
        logger.info("âœ… ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰æä¾›ã®æƒ…å ±ã‚’ä½¿ç”¨ã€‚DBã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        researcher_data = request.researcher_info.dict(exclude_unset=True, by_alias=False) # by_alias=Falseã§Pythonã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ä½¿ã†
    else:
        logger.warning(f"âš ï¸ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®æƒ…å ±æä¾›ãªã—ã€‚DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™: {request.researchmap_url}")
        researcher_data = get_researcher_data_by_url(request.researchmap_url)
    
    if not researcher_data:
        error_msg = "æŒ‡å®šã•ã‚ŒãŸURLã®ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        logger.error(error_msg + f" URL: {request.researchmap_url}")
        return JSONResponse(
            status_code=404,
            content={"status": "error", "error": error_msg}
        )
        
    try:
        from evaluation_system import UniversalResearchEvaluator
        evaluator = UniversalResearchEvaluator()
        
        summary_text = await evaluator.generate_single_summary(researcher_data, request.query)
        
        if summary_text:
            logger.info(f"âœ… AIè¦ç´„ç”ŸæˆæˆåŠŸ: {request.researchmap_url}")
            return {"status": "success", "summary": summary_text}
        else:
            raise Exception("LLMã‹ã‚‰ã®è¦ç´„å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        logger.error(f"âŒ AIè¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "error": f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}
        )

# --- ã“ã“ã¾ã§ãŒå¾©å…ƒã•ã‚ŒãŸæ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---

@app.post("/api/search", response_model=SearchResponse)
async def search_researchers(request: SearchRequest):
    """
    ç ”ç©¶è€…æ¤œç´¢APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆå®Ÿéš›ã®æ¤œç´¢ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    start_time = time.time()
    
    logger.info(f"ğŸ” æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: query={request.query}, method={request.method}, max_results={request.max_results}")
    if request.university_filter:
        logger.info(f"ğŸ« å¤§å­¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {request.university_filter}")
    if request.exclude_keywords:
        logger.info(f"ğŸš« é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {request.exclude_keywords}")

    try:
        from real_search import perform_real_search
        result = await perform_real_search(request)
        
        if result["status"] == "success":
            logger.info(f"âœ… å®Ÿéš›ã®æ¤œç´¢æˆåŠŸ: {len(result.get('results', []))}ä»¶")
            return SearchResponse(**result)
        else:
            logger.warning(f"âš ï¸ å®Ÿéš›ã®æ¤œç´¢å¤±æ•—ã€ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {result.get('error_message')}")
            
    except Exception as e:
        import traceback
        logger.error(f"âš ï¸ å®Ÿéš›ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ã€ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}\n{traceback.format_exc()}")
    
    # ãƒ¢ãƒƒã‚¯æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    mock_results = []
    expanded_info = None
    
    if request.query:
        if request.use_llm_expansion and request.method == "keyword":
            mock_expanded_keywords = [ request.query, f"{request.query}ç ”ç©¶", f"{request.query}æŠ€è¡“", "æœ€æ–°æŠ€è¡“", "å…ˆç«¯ç ”ç©¶" ]
            expanded_info = {
                "original_query": request.query,
                "expanded_keywords": mock_expanded_keywords,
                "expanded_query": " ".join(mock_expanded_keywords)
            }
        
        mock_researchers = [
            { "name_ja": f"ç ”ç©¶è€…Aï¼ˆé–¢é€£: {request.query}ï¼‰", "main_affiliation_name_ja": "ã‚µãƒ³ãƒ—ãƒ«å¤§å­¦", "research_keywords_ja": f"{request.query}, æ©Ÿæ¢°å­¦ç¿’", "researchmap_url": "https://researchmap.jp/sample1", "distance": 0.1234 },
            { "name_ja": f"ç ”ç©¶è€…Bï¼ˆé–¢é€£: {request.query}ï¼‰", "main_affiliation_name_ja": "å…ˆç«¯æŠ€è¡“ç ”ç©¶æ‰€", "research_keywords_ja": f"{request.query}, å¿œç”¨ç ”ç©¶", "researchmap_url": "https://researchmap.jp/sample2", "distance": 0.2156 }
        ]
        
        mock_results = mock_researchers[:min(request.max_results, len(mock_researchers))]
        
        if request.use_llm_summary:
            for result in mock_results:
                result["llm_summary"] = f"ã“ã®ç ”ç©¶è€…ã¯ã€Œ{request.query}ã€ã«é–¢ã—ã¦æ·±ã„å°‚é–€çŸ¥è­˜ã‚’æœ‰ã—ã¦ã„ã¾ã™ã€‚"
    
    execution_time = time.time() - start_time
    
    executed_query_info = f"ãƒ¢ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œï¼ˆå®Ÿéš›ã®æ¤œç´¢ã¯æº–å‚™ä¸­ï¼‰"
    
    return SearchResponse(
        status="success",
        query=request.query,
        method=request.method,
        results=[ResearcherResult(**result) for result in mock_results],
        total=len(mock_results),
        execution_time=execution_time,
        executed_query_info=executed_query_info,
        expanded_info=expanded_info
    )

@app.post("/api/analyze-researcher", response_model=AnalysisResponse)
async def analyze_researcher(request: AnalyzeRequest):
    """
    ResearchMap APIã‚’ä½¿ç”¨ã—ãŸç ”ç©¶è€…è©³ç´°åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    start_time = time.time()
    logger.info(f"ğŸ” ç ”ç©¶è€…åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: {request.researchmap_url}, query: {request.query}")
    try:
        from researchmap.analyzer import ResearchMapAnalyzer
        analyzer = ResearchMapAnalyzer()
        result = await analyzer.analyze_researcher(
            researchmap_url=request.researchmap_url,
            query=request.query,
            basic_info=request.researcher_basic_info,
            include_keyword_map=request.include_keyword_map
        )
        logger.info(f"âœ… ç ”ç©¶è€…åˆ†æå®Œäº†: {(time.time() - start_time):.2f}ç§’")
        return AnalysisResponse(**result)
    except Exception as e:
        logger.error(f"âŒ ç ”ç©¶è€…åˆ†æã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")
        return AnalysisResponse(status="error", error=f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", analysis=None)

# =============================================================================
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# =============================================================================

@app.post("/api/temp-projects", response_model=TempProject)
async def create_temp_project(request: ProjectCreateRequest):
    """ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
    try:
        project = project_manager.create_temp_project(request)
        return project
    except Exception as e:
        logger.error(f"âŒ ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/temp-projects")
async def list_temp_projects(user_id: Optional[str] = Query(None)):
    """ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§ã‚’å–å¾—"""
    try:
        projects = project_manager.list_temp_projects(user_id)
        return {"status": "success", "projects": projects, "total": len(projects)}
    except Exception as e:
        logger.error(f"âŒ ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/temp-projects/{project_id}")
async def get_temp_project(project_id: str):
    """ç‰¹å®šã®ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
    try:
        project = project_manager.get_temp_project(project_id)
        if not project:
            raise HTTPException(status_code=404, detail="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "success", "project": project}
    except Exception as e:
        logger.error(f"âŒ ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/temp-projects/{project_id}/researchers")
async def add_researcher_to_project(project_id: str, request: ResearcherSelectionRequest):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ç ”ç©¶è€…ã‚’è¿½åŠ """
    try:
        researcher_data = request.dict()
        success = project_manager.add_researcher_to_project(project_id, researcher_data)
        if not success:
            raise HTTPException(status_code=400, detail="ç ”ç©¶è€…ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return {"status": "success", "message": "ç ”ç©¶è€…ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"âŒ ç ”ç©¶è€…è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/temp-projects/{project_id}/researchers/{researcher_name}")
async def remove_researcher_from_project(project_id: str, researcher_name: str):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ç ”ç©¶è€…ã‚’å‰Šé™¤"""
    try:
        success = project_manager.remove_researcher_from_project(project_id, researcher_name)
        if not success:
            raise HTTPException(status_code=404, detail="ç ”ç©¶è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "success", "message": "ç ”ç©¶è€…ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å‰Šé™¤ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"âŒ ç ”ç©¶è€…å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/temp-projects/{project_id}/matching-request")
async def submit_matching_request(project_id: str, request: MatchingRequest):
    """ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã‚’é€ä¿¡"""
    try:
        result = project_manager.submit_matching_request(project_id, request)
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error"))
        return {"status": "success", "result": result}
    except Exception as e:
        logger.error(f"âŒ ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/temp-projects/{project_id}/status")
async def update_project_status(project_id: str, status: str = Query(...)):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°"""
    try:
        success = project_manager.update_project_status(project_id, status)
        if not success:
            raise HTTPException(status_code=404, detail="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "success", "message": f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’{status}ã«æ›´æ–°ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/temp-projects/{project_id}/researchers/{researcher_name}/memo")
async def update_researcher_memo(project_id: str, researcher_name: str, memo: str = Query(...)):
    """ç ”ç©¶è€…ã®ãƒ¡ãƒ¢ã‚’æ›´æ–°"""
    try:
        success = project_manager.update_researcher_memo(project_id, researcher_name, memo)
        if not success:
            raise HTTPException(status_code=404, detail="ç ”ç©¶è€…ã¾ãŸã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "success", "message": "ãƒ¡ãƒ¢ã‚’æ›´æ–°ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"âŒ ç ”ç©¶è€…ãƒ¡ãƒ¢æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/temp-projects/{project_id}")
async def delete_temp_project(project_id: str):
    """ä»®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤"""
    try:
        success = project_manager.delete_temp_project(project_id)
        if not success:
            raise HTTPException(status_code=404, detail="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {"status": "success", "message": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ"}
    except Exception as e:
        logger.error(f"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {exc}")
    import traceback
    logger.error(traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={"detail": f"å†…éƒ¨ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(exc)}"}
    )

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.environ.get("PORT", 8000))
    print(f"ğŸš€ Starting Research API v2.1.1 (æœ€çµ‚ä¿®æ­£ç‰ˆ) on port {port}")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
