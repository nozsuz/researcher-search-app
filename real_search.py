"""
å®Ÿéš›ã®ç ”ç©¶è€…æ¤œç´¢æ©Ÿèƒ½ï¼ˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ»é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿèƒ½çµ±åˆç‰ˆï¼‰
BigQuery + Vertex AI + è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸæœ¬æ ¼çš„ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
"""

import logging
import time
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from google.cloud import bigquery
from vertexai.language_models import TextGenerationModel, TextEmbeddingModel
from vertexai.generative_models import GenerativeModel
import numpy as np
from evaluation_system import UniversalResearchEvaluator

logger = logging.getLogger(__name__)

# è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¿æŒ
evaluator = UniversalResearchEvaluator()

def is_young_researcher(researcher_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """
    è‹¥æ‰‹ç ”ç©¶è€…ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä¿®æ­£ãƒ»æ–‡å­—åŒ–ã‘å¯¾ç­–ç‰ˆï¼‰
    """
    reasons = []
    name = researcher_data.get('name_ja', 'Unknown')
    logger.debug(f"ğŸ” è‹¥æ‰‹ç ”ç©¶è€…åˆ¤å®šé–‹å§‹: {name}")

    # --- ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
    profile_ja = (researcher_data.get('profile_ja', '') or '').lower()
    job_ja = (researcher_data.get('main_affiliation_job_ja', '') or '').lower()
    job_title_ja = (researcher_data.get('main_affiliation_job_title_ja', '') or '').lower()
    job_en = (researcher_data.get('main_affiliation_job_en', '') or '').lower()
    job_title_en = (researcher_data.get('main_affiliation_job_title_en', '') or '').lower()
    
    combined_job_info = f"{job_ja} {job_title_ja} {job_en} {job_title_en}"
    
    # --- åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾© ---
    young_positions_ja = ['åŠ©æ•™', 'å‡†æ•™æˆ', 'åšå£«ç ”ç©¶å“¡', 'ãƒã‚¹ãƒ‰ã‚¯', 'ç ”ç©¶å“¡', 'ç‰¹ä»»åŠ©æ•™', 'ç‰¹ä»»å‡†æ•™æˆ', 'åŠ©æ‰‹', 'è¬›å¸«', 'ç‰¹ä»»ç ”ç©¶å“¡', 'åšå£«å¾ŒæœŸèª²ç¨‹', 'åšå£«èª²ç¨‹', 'ãƒã‚¹ãƒˆãƒ‰ã‚¯ãƒˆãƒ©ãƒ«', 'æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡', 'jspsç‰¹åˆ¥ç ”ç©¶å“¡', 'ç‰¹åˆ¥ç ”ç©¶å“¡', 'åšå£«å­¦ç”Ÿ', 'å¤§å­¦é™¢ç”Ÿ', 'åŒ»å“¡']
    young_positions_en = ['assistant', 'associate professor', 'postdoc', 'researcher', 'fellow', 'doctoral', 'phd student', 'graduate student', 'research associate', 'post-doctoral', 'jsps fellow']
    senior_positions_ja = ['æ•™æˆ', 'åèª‰æ•™æˆ', 'å®¢å“¡æ•™æˆ', 'ç‰¹ä»»æ•™æˆ', 'ä¸»å¸­ç ”ç©¶å“¡', 'çµ±æ‹¬', 'ä»£è¡¨å–ç· å½¹', 'ç¤¾é•·', 'æ‰€é•·', 'ã‚»ãƒ³ã‚¿ãƒ¼é•·']
    senior_positions_en = ['professor', 'emeritus', 'director', 'principal', 'chief', 'ceo', 'president']
    
    # --- 1. é™¤å¤–æ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯ (æœ€å„ªå…ˆ) ---
    for pos in senior_positions_ja:
        if pos in combined_job_info:
            reasons = [f"é™¤å¤–æ¡ä»¶(\u8077\u4f4d): {pos}"]
            logger.debug(f"ğŸ¯ è‹¥æ‰‹åˆ¤å®šçµæœ: {name} - False - {reasons}")
            return False, reasons
            
    for pos in senior_positions_en:
        if pos in combined_job_info and 'associate professor' not in combined_job_info:
            reasons = [f"é™¤å¤–æ¡ä»¶(\u8077\u4f4d,è‹±): {pos}"]
            logger.debug(f"ğŸ¯ è‹¥æ‰‹åˆ¤å®šçµæœ: {name} - False - {reasons}")
            return False, reasons

    exclusion_keywords_profile = ['é€€è·', 'å…ƒæ•™æˆ', 'å…ƒæ‰€é•·', 'é¡§å•', 'ç†äº‹é•·', 'å­¦é•·', 'ç·é•·']
    for keyword in exclusion_keywords_profile:
        if keyword in profile_ja:
            reasons = [f"é™¤å¤–æ¡ä»¶(çµŒæ­´): {keyword}"]
            logger.debug(f"ğŸ¯ è‹¥æ‰‹åˆ¤å®šçµæœ: {name} - False - {reasons}")
            return False, reasons

    # --- 2. è‹¥æ‰‹åˆ¤å®š (è·ä½ã‚’å„ªå…ˆ) ---
    for pos in young_positions_ja:
        if pos in combined_job_info:
            reasons.append(f"\u8077\u4f4d: {pos}")
            break
    if not reasons:
        for pos in young_positions_en:
            if pos in combined_job_info:
                reasons.append(f"\u8077\u4f4d(è‹±): {pos}")
                break

    # --- 3. ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰ã®æ¨æ¸¬ (è·ä½ã§åˆ¤å®šã§ããªã‹ã£ãŸå ´åˆ) ---
    if not reasons:
        current_position_patterns = [r'\d{4}å¹´\s*-\s*(.+)', r'\d{4}å¹´\s*ï½\s*(.+)', r'\d{4}å¹´\s*ã‹ã‚‰\s*(.+)', r'ç¾åœ¨\s*[ï¼š:]?\s*(.+)']
        for pattern in current_position_patterns:
            match = re.search(pattern, profile_ja)
            if match:
                position_text = match.group(1).lower()
                if not any(sp in position_text for sp in senior_positions_ja):
                    for pos in young_positions_ja:
                        if pos in position_text:
                            reasons.append(f"ç¾è·(ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«): {pos}")
                            break
                if reasons: break
        
        if not reasons:
            current_year = datetime.now().year
            phd_patterns = [r'(\d{4})å¹´.*?åšå£«.*?å–å¾—', r'(\d{4})å¹´.*?ph\\.?d\\.?', r'åšå£«.*?(\d{4})å¹´', r'ph\\.?d\\.?.*?(\d{4})', r'(\d{4})å¹´.*?å­¦ä½', r'(\d{4})å¹´.*?åšå£«èª²ç¨‹.*?ä¿®äº†']
            for pattern in phd_patterns:
                match = re.search(pattern, profile_ja)
                if match:
                    year_str = match.group(1)
                    if year_str and year_str.isdigit():
                        year = int(year_str)
                        years_since = current_year - year
                        if 0 <= years_since <= 15:
                            reasons.append(f"åšå£«å–å¾—: {year}å¹´ï¼ˆ{years_since}å¹´å‰ï¼‰")
                            break
            
            if not reasons:
                age_patterns = [r'(\d{2})æ­³', r'(\d{4})å¹´ç”Ÿã¾ã‚Œ', r'(\d{4})å¹´.*?èª•ç”Ÿ']
                for pattern in age_patterns:
                    match = re.search(pattern, profile_ja)
                    if match:
                        age_text = match.group(1)
                        if age_text and age_text.isdigit():
                            if 'æ­³' in pattern:
                                age = int(age_text)
                                if 25 <= age <= 45: reasons.append(f"å¹´é½¢: {age}æ­³")
                            else:
                                birth_year = int(age_text)
                                age = current_year - birth_year
                                if 25 <= age <= 45: reasons.append(f"ç”Ÿå¹´: {birth_year}å¹´ï¼ˆ{age}æ­³ï¼‰")
                        if reasons: break
            
            if not reasons:
                young_keywords = ['è‹¥æ‰‹', 'æ–°é€²æ°—é‹­', 'early career', 'åšå£«èª²ç¨‹', 'åšå£«å–å¾—', 'ã‚­ãƒ£ãƒªã‚¢åˆæœŸ', 'ç ”ç©¶å“¡ã¨ã—ã¦', 'æ¡ç”¨ã•ã‚Œ', 'ç€ä»»', 'åšå£«å·å–å¾—', 'ph.d.å–å¾—', 'å­¦æŒ¯', 'jsps', 'è‚²å¿—è³', 'è‹¥æ‰‹ç ”ç©¶è€…è³', 'å¥¨åŠ±è³']
                for keyword in young_keywords:
                    if keyword in profile_ja:
                        reasons.append(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
                        break

    is_young = len(reasons) > 0
    logger.debug(f"ğŸ¯ è‹¥æ‰‹åˆ¤å®šçµæœ: {name} - {is_young} - {reasons}")
    return is_young, list(set(reasons))


async def perform_real_search(request) -> Dict[str, Any]:
    """
    ç ”ç©¶è€…æ¤œç´¢ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ç‰ˆï¼‰
    """
    start_time = time.time()
    try:
        logger.info(f"ğŸ” å®Ÿéš›ã®æ¤œç´¢é–‹å§‹: {request.query}, method: {request.method}")
        
        # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æº–å‚™ ---
        young_researcher_filter = getattr(request, 'young_researcher_filter', False)
        university_filter = getattr(request, 'university_filter', None)
        exclude_keywords = getattr(request, 'exclude_keywords', None)
        
        logger.info(f"ğŸ“Š è‹¥æ‰‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {'ON' if young_researcher_filter else 'OFF'}")
        if university_filter: logger.info(f"ğŸ« å¤§å­¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {university_filter}")
        if exclude_keywords: logger.info(f"ğŸš« é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {exclude_keywords}")

        # --- GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæº–å‚™ ---
        from gcp_auth import get_bigquery_client, is_vertex_ai_ready
        bq_client = get_bigquery_client()
        if not bq_client:
            raise Exception("BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        vertex_ai_available = is_vertex_ai_ready()
        if (request.method == "semantic" or request.use_llm_expansion) and not vertex_ai_available:
            logger.warning("âš ï¸ Vertex AIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            request.method = "keyword"
            request.use_llm_expansion = False

        # --- ã‚¯ã‚¨ãƒªæ‹¡å¼µ (å¿…è¦ãªå ´åˆ) ---
        search_query = request.query.strip()
        expanded_info = None
        if request.use_llm_expansion and request.method == "keyword":
            try:
                expansion_result = await expand_query_with_llm(search_query)
                if expansion_result and expansion_result["expanded_keywords"] != [search_query]:
                    expanded_info = expansion_result
                    search_query = expansion_result["expanded_query"]
                    logger.info(f"ğŸ§  ã‚¯ã‚¨ãƒªæ‹¡å¼µå®Ÿè¡Œ: {search_query}")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMã‚¯ã‚¨ãƒªæ‹¡å¼µå¤±æ•—: {e}")

        # --- æ¤œç´¢å®Ÿè¡Œ ---
        # å„æ¤œç´¢é–¢æ•°å†…ã§ is_young_researcher ã®åˆ¤å®šãŒè¡Œã‚ã‚Œã‚‹
        if request.method == "semantic":
            results = await semantic_search_with_embedding(bq_client, search_query, request.max_results, university_filter, exclude_keywords)
        else:
            results = await keyword_search(bq_client, search_query, request.max_results, university_filter, exclude_keywords)
        
        logger.info(f"ğŸ“Š åˆæœŸæ¤œç´¢çµæœ: {len(results)}ä»¶")

        # --- è‹¥æ‰‹ç ”ç©¶è€…ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ã“ã“ãŒé‡è¦) ---
        # æ¤œç´¢çµæœã«å¯¾ã—ã¦ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹
        if young_researcher_filter:
            logger.info(f"ğŸŒŸ è‹¥æ‰‹ç ”ç©¶è€…ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ")
            
            # is_young_researcherãŒTrueã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡º
            filtered_results = [r for r in results if r.get('is_young_researcher', False)]
            
            logger.info(f"ğŸŒŸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ: {len(results)}ä»¶ â†’ {len(filtered_results)}ä»¶")
            results = filtered_results

        # --- AIè¦ç´„ (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®çµæœã«å¯¾ã—ã¦å®Ÿè¡Œ) ---
        if request.use_llm_summary and results and vertex_ai_available:
            try:
                results = await add_llm_summaries(results, request.query)
                logger.info("ğŸ¤– AIè¦ç´„ã‚’è¿½åŠ å®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ AIè¦ç´„ç”Ÿæˆå¤±æ•—: {e}")

        # --- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ ---
        execution_time = time.time() - start_time
        executed_query_info = f"å®Ÿéš›ã®GCPæ¤œç´¢å®Ÿè¡Œ (æ–¹æ³•: {request.method}, å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’)"
        
        return {
            "status": "success",
            "query": request.query,
            "method": request.method,
            "results": results,
            "total": len(results),
            "execution_time": execution_time,
            "executed_query_info": executed_query_info,
            "expanded_info": expanded_info
        }

    except Exception as e:
        logger.error(f"âŒ å®Ÿéš›ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
        return {
            "status": "error",
            "error_message": str(e),
            "execution_time": time.time() - start_time
        }

# â–¼â–¼â–¼ ã“ã®é–¢æ•°ã‚’ã¾ã‚‹ã”ã¨ç½®ãæ›ãˆã¦ãã ã•ã„ â–¼â–¼â–¼
async def semantic_search_with_embedding(bq_client: bigquery.Client, query: str, max_results: int, university_filter: Optional[List[str]] = None, exclude_keywords: Optional[List[str]] = None) -> List[Dict]:
    """
    å®Ÿéš›ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆäº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ–¹å¼ã®ã€å®Œå…¨ä¿®æ­£ç‰ˆã€‘ï¼‰
    """
    query_embedding = None
    try:
        logger.info(f"ğŸ” ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆäº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ä¿®æ­£ç‰ˆã€‘ï¼‰å®Ÿè¡Œ: {query}")
        
        # 1. ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embedding_model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
        query_embeddings = embedding_model.get_embeddings([query])
        query_embedding = query_embeddings[0].values
        
        expected_dimensions = 768
        if len(query_embedding) != expected_dimensions:
            if len(query_embedding) > expected_dimensions:
                query_embedding = query_embedding[:expected_dimensions]
            else:
                query_embedding = query_embedding + [0.0] * (expected_dimensions - len(query_embedding))
        
        query_embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
        
        # 2. å¤§å­¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ï¼ˆäº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
        university_condition = ""
        if university_filter and len(university_filter) > 0:
            try:
                from university_normalizer_fixed import get_university_normalization_sql
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                university_list = ",".join([f"'{univ}'" for univ in safe_universities])
                normalization_sql = get_university_normalization_sql("main_affiliation_name_ja")
                university_condition = f" AND ({normalization_sql}) IN ({university_list})"
            except Exception as e:
                logger.warning(f"âš ï¸ å¤§å­¦æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã€ã‚·ãƒ³ãƒ—ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨: {e}")
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                like_conditions = [f"main_affiliation_name_ja LIKE '%{univ}%'" for univ in safe_universities]
                university_condition = f" AND ({' OR '.join(like_conditions)})"

        # 3. é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¡ä»¶ï¼ˆäº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
        exclude_where_clause = ""
        if exclude_keywords:
            conditions = []
            for keyword in exclude_keywords:
                safe_keyword = keyword.replace("'", "''")
                # `base`ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
                conditions.append(f"""
                    NOT (
                        LOWER(research_keywords_ja) LIKE LOWER('%{safe_keyword}%') OR
                        LOWER(research_fields_ja) LIKE LOWER('%{safe_keyword}%') OR
                        LOWER(profile_ja) LIKE LOWER('%{safe_keyword}%')
                    )
                """)
            if conditions:
                exclude_where_clause = f"WHERE {' AND '.join(conditions)}"

        # 4. äº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†SQLã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        top_k_for_search = max(50, max_results * 5)

        sql_query_semantic = f"""
        WITH vector_results AS (
            SELECT
                base.*,  -- ã“ã“ã§baseæ§‹é€ ã‚’å±•é–‹ã™ã‚‹
                distance
            FROM
                VECTOR_SEARCH(
                    (SELECT * FROM `apt-rope-217206.researcher_data.rd_250524`
                     WHERE ARRAY_LENGTH(embedding) > 0{university_condition}),
                    'embedding',
                    (SELECT {query_embedding_str} AS query_vector),
                    top_k => @top_k_for_search,
                    distance_type => 'COSINE'
                )
        )
        SELECT *
        FROM vector_results
        {exclude_where_clause}
        ORDER BY distance ASC
        LIMIT @max_results
        """
        
        logger.info(f"Generated SQL for Semantic Search:\n{sql_query_semantic}")
        
        try:
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("top_k_for_search", "INT64", top_k_for_search),
                    bigquery.ScalarQueryParameter("max_results", "INT64", max_results),
                ]
            )
            df = bq_client.query(sql_query_semantic, job_config=job_config).to_dataframe()
            
            if len(df) > 0:
                results = []
                # ã“ã®ä¿®æ­£ã«ã‚ˆã‚Šã€dfã«ã¯æ—¢ã«å±•é–‹ã•ã‚ŒãŸã‚«ãƒ©ãƒ ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–
                for idx, row in df.iterrows():
                    result = row.to_dict()
                    result["distance"] = row["distance"] # â† ã“ã®è¡Œã‚’è¿½åŠ 
                    is_young, young_reasons = is_young_researcher(result)
                    result["is_young_researcher"] = is_young
                    result["young_researcher_reasons"] = young_reasons
                    results.append(result)
                
                logger.info(f"âœ… ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: {len(results)}ä»¶")
                return results
            else:
                logger.info("æ¤œç´¢çµæœãŒç©ºã§ã™ã€‚")
                return []
                
        except Exception as e:
            logger.error(f"BigQueryã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            logger.info("ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return await semantic_search_realtime_fallback(bq_client, query, query_embedding, max_results, university_filter, exclude_keywords)
        
    except Exception as e:
        logger.error(f"âŒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        logger.info("ğŸ”„ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return await keyword_search(bq_client, query, max_results, university_filter, exclude_keywords)

async def semantic_search_realtime_fallback(bq_client: bigquery.Client, query: str, query_embedding: List[float], max_results: int, university_filter: Optional[List[str]] = None, exclude_keywords: Optional[List[str]] = None) -> List[Dict]:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        logger.info(f"ğŸ” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ")
        first_keyword = query.split()[0] if query.split() else query
        university_condition = ""
        if university_filter and len(university_filter) > 0:
            try:
                from university_normalizer_fixed import get_university_normalization_sql
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                university_list = ",".join([f"'{univ}'" for univ in safe_universities])
                normalization_sql = get_university_normalization_sql("main_affiliation_name_ja")
                university_condition = f" AND ({normalization_sql}) IN ({university_list})"
            except Exception as e:
                logger.warning(f"âš ï¸ å¤§å­¦æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã€ã‚·ãƒ³ãƒ—ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨: {e}")
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                like_conditions = [f"main_affiliation_name_ja LIKE '%{univ}%'" for univ in safe_universities]
                university_condition = f" AND ({' OR '.join(like_conditions)})"
        exclude_condition = ""
        if exclude_keywords:
            conditions = []
            for keyword in exclude_keywords:
                safe_keyword = keyword.replace("'", "''")
                conditions.append(f""" NOT ( LOWER(research_keywords_ja) LIKE LOWER('%{safe_keyword}%') OR LOWER(research_fields_ja) LIKE LOWER('%{safe_keyword}%') OR LOWER(profile_ja) LIKE LOWER('%{safe_keyword}%') ) """)
            if conditions: exclude_condition = f" AND {' AND '.join(conditions)}"
        search_sql = f""" SELECT name_ja, name_en, main_affiliation_name_ja, main_affiliation_name_en, main_affiliation_job_ja, main_affiliation_job_title_ja, main_affiliation_job_en, main_affiliation_job_title_en, research_keywords_ja, research_fields_ja, profile_ja, paper_title_ja_first, project_title_ja_first, researchmap_url FROM `apt-rope-217206.researcher_data.rd_250524` WHERE ( research_keywords_ja IS NOT NULL OR research_fields_ja IS NOT NULL OR profile_ja IS NOT NULL ) AND ( LOWER(research_keywords_ja) LIKE LOWER('%{first_keyword}%') OR LOWER(research_fields_ja) LIKE LOWER('%{first_keyword}%') OR LOWER(profile_ja) LIKE LOWER('%{first_keyword}%') ){university_condition}{exclude_condition} LIMIT {max_results * 5} """
        query_job = bq_client.query(search_sql)
        candidates = []
        for row in query_job:
            researcher_text = ""
            if row.research_keywords_ja: researcher_text += row.research_keywords_ja + " "
            if row.research_fields_ja: researcher_text += row.research_fields_ja + " "
            if row.profile_ja: researcher_text += row.profile_ja[:200] + " "
            candidates.append({ "data": { "name_ja": row.name_ja, "name_en": row.name_en, "main_affiliation_name_ja": row.main_affiliation_name_ja, "main_affiliation_name_en": row.main_affiliation_name_en, "main_affiliation_job_ja": row.main_affiliation_job_ja, "main_affiliation_job_title_ja": row.main_affiliation_job_title_ja, "main_affiliation_job_en": row.main_affiliation_job_en, "main_affiliation_job_title_en": row.main_affiliation_job_title_en, "research_keywords_ja": row.research_keywords_ja, "research_fields_ja": row.research_fields_ja, "profile_ja": row.profile_ja, "paper_title_ja_first": row.paper_title_ja_first, "project_title_ja_first": row.project_title_ja_first, "researchmap_url": row.researchmap_url }, "text": researcher_text.strip() })
        if not candidates:
            logger.info("ğŸ“Š ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []
        logger.info(f"ğŸ“Š ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å€™è£œ: {len(candidates)}å")
        embedding_model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
        candidate_texts = [candidate["text"] for candidate in candidates if candidate["text"]]
        if not candidate_texts:
            logger.info("ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return []
        batch_size = 5
        candidate_embeddings = []
        for i in range(0, len(candidate_texts), batch_size):
            batch_texts = candidate_texts[i:i+batch_size]
            try:
                batch_embeddings = embedding_model.get_embeddings(batch_texts)
                candidate_embeddings.extend([emb.values for emb in batch_embeddings])
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒãƒƒãƒ{i//batch_size + 1}ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—: {e}")
                candidate_embeddings.extend([[0.0] * len(query_embedding)] * len(batch_texts))
        results_with_similarity = []
        for i, candidate in enumerate(candidates[:len(candidate_embeddings)]):
            if i >= len(candidate_embeddings): continue
            candidate_embedding = candidate_embeddings[i]
            similarity = calculate_cosine_similarity(query_embedding, candidate_embedding)
            result = candidate["data"].copy()
            result["distance"] = 1.0 - similarity
            result["similarity"] = similarity
            is_young, young_reasons = is_young_researcher(result)
            result["is_young_researcher"] = is_young
            result["young_researcher_reasons"] = young_reasons
            results_with_similarity.append(result)
        results_with_similarity.sort(key=lambda x: x["distance"])
        final_results = results_with_similarity[:max_results]
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Œäº†: {len(final_results)}ä»¶")
        if final_results: logger.info(f"ğŸ“Š æœ€å°è·é›¢: {final_results[0]['distance']:.4f}")
        return final_results
    except Exception as e:
        logger.error(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def calculate_cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        vec1, vec2 = np.array(vec1), np.array(vec2)
        norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0: return 0.0
        similarity = np.dot(vec1, vec2) / (norm1 * norm2)
        return float(similarity)
    except Exception as e:
        logger.warning(f"âš ï¸ ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0.0

async def keyword_search(bq_client: bigquery.Client, query: str, max_results: int, university_filter: Optional[List[str]] = None, exclude_keywords: Optional[List[str]] = None) -> List[Dict]:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢å®Ÿè¡Œ: {query}")
        keywords = [kw.strip() for kw in query.split() if kw.strip()]
        logger.info(f"ğŸ“ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        like_conditions = []
        for keyword in keywords:
            safe_keyword = keyword.replace("'", "''")
            like_conditions.extend([ f"LOWER(research_keywords_ja) LIKE LOWER('%{safe_keyword}%')", f"LOWER(research_fields_ja) LIKE LOWER('%{safe_keyword}%')", f"LOWER(profile_ja) LIKE LOWER('%{safe_keyword}%')", f"LOWER(name_ja) LIKE LOWER('%{safe_keyword}%')", f"LOWER(paper_title_ja_first) LIKE LOWER('%{safe_keyword}%')", f"LOWER(project_title_ja_first) LIKE LOWER('%{safe_keyword}%')" ])
        where_clause = " OR ".join(like_conditions)
        relevance_scores = []
        for keyword in keywords:
            safe_keyword = keyword.replace("'", "''")
            relevance_scores.append(f""" ( CASE WHEN LOWER(name_ja) LIKE LOWER('%{safe_keyword}%') THEN 10 ELSE 0 END + CASE WHEN LOWER(research_keywords_ja) LIKE LOWER('%{safe_keyword}%') THEN 8 ELSE 0 END + CASE WHEN LOWER(research_fields_ja) LIKE LOWER('%{safe_keyword}%') THEN 6 ELSE 0 END + CASE WHEN LOWER(paper_title_ja_first) LIKE LOWER('%{safe_keyword}%') THEN 5 ELSE 0 END + CASE WHEN LOWER(project_title_ja_first) LIKE LOWER('%{safe_keyword}%') THEN 5 ELSE 0 END + CASE WHEN LOWER(profile_ja) LIKE LOWER('%{safe_keyword}%') THEN 4 ELSE 0 END ) """)
        total_relevance_score = " + ".join(relevance_scores) if relevance_scores else "0"
        university_condition = ""
        if university_filter and len(university_filter) > 0:
            try:
                from university_normalizer_fixed import get_university_normalization_sql
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                university_list = ",".join([f"'{univ}'" for univ in safe_universities])
                normalization_sql = get_university_normalization_sql("main_affiliation_name_ja")
                university_condition = f" AND ({normalization_sql}) IN ({university_list})"
            except Exception as e:
                logger.warning(f"âš ï¸ å¤§å­¦æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã€ã‚·ãƒ³ãƒ—ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨: {e}")
                safe_universities = [univ.replace("'", "''") for univ in university_filter]
                like_conditions = [f"main_affiliation_name_ja LIKE '%{univ}%'" for univ in safe_universities]
                university_condition = f" AND ({' OR '.join(like_conditions)})"
        exclude_condition = ""
        if exclude_keywords:
            conditions = []
            for keyword in exclude_keywords:
                safe_keyword = keyword.replace("'", "''")
                conditions.append(f""" NOT ( LOWER(research_keywords_ja) LIKE LOWER('%{safe_keyword}%') OR LOWER(research_fields_ja) LIKE LOWER('%{safe_keyword}%') OR LOWER(profile_ja) LIKE LOWER('%{safe_keyword}%') ) """)
            if conditions: exclude_condition = f" AND {' AND '.join(conditions)}"
        search_sql = f""" SELECT name_ja, name_en, main_affiliation_name_ja, main_affiliation_name_en, main_affiliation_job_ja, main_affiliation_job_title_ja, main_affiliation_job_en, main_affiliation_job_title_en, research_keywords_ja, research_fields_ja, profile_ja, paper_title_ja_first, project_title_ja_first, researchmap_url, ({total_relevance_score}) as relevance_score FROM `apt-rope-217206.researcher_data.rd_250524` WHERE ({where_clause}){university_condition}{exclude_condition} ORDER BY relevance_score DESC, name_ja LIMIT {max_results} """
        query_job = bq_client.query(search_sql)
        results = []
        for row in query_job:
            researcher_data = { "name_ja": row.name_ja, "name_en": row.name_en, "main_affiliation_name_ja": row.main_affiliation_name_ja, "main_affiliation_name_en": row.main_affiliation_name_en, "main_affiliation_job_ja": row.main_affiliation_job_ja, "main_affiliation_job_title_ja": row.main_affiliation_job_title_ja, "main_affiliation_job_en": row.main_affiliation_job_en, "main_affiliation_job_title_en": row.main_affiliation_job_title_en, "research_keywords_ja": row.research_keywords_ja, "research_fields_ja": row.research_fields_ja, "profile_ja": row.profile_ja, "paper_title_ja_first": row.paper_title_ja_first, "project_title_ja_first": row.project_title_ja_first, "researchmap_url": row.researchmap_url, "relevance_score": float(row.relevance_score) if row.relevance_score else None }
            is_young, young_reasons = is_young_researcher(researcher_data)
            researcher_data["is_young_researcher"] = is_young
            researcher_data["young_researcher_reasons"] = young_reasons
            if 'å¾Œè—¤' in researcher_data.get('name_ja', '') or 'å°æ¾' in researcher_data.get('name_ja', ''):
                logger.info(f"ğŸ” æ¤œç´¢ - {researcher_data.get('name_ja')}æ°ã®ãƒ‡ãƒ¼ã‚¿: ")
                logger.info(f"  - main_affiliation_job_ja: {researcher_data.get('main_affiliation_job_ja', 'NULL/MISSING')}")
                logger.info(f"  - main_affiliation_job_title_ja: {researcher_data.get('main_affiliation_job_title_ja', 'NULL/MISSING')}")
                logger.info(f"  - is_young_researcher: {is_young}")
                logger.info(f"  - young_researcher_reasons: {young_reasons}")
                logger.info(f"  - profile_ja[:300]: {str(researcher_data.get('profile_ja', ''))[:300]}")
                logger.info(f"  - å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {list(researcher_data.keys())}")
                if 'main_affiliation_job_ja' not in researcher_data: logger.warning(f"  âš ï¸ main_affiliation_job_ja ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼")
            results.append(researcher_data)
        logger.info(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢å®Œäº†: {len(results)}ä»¶")
        if results and len(results) > 0:
            first_result = results[0]
            logger.info(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿:")
            logger.info(f"  - name_ja: {first_result.get('name_ja', 'N/A')}")
            logger.info(f"  - is_young_researcher: {first_result.get('is_young_researcher', 'MISSING')}")
            logger.info(f"  - young_researcher_reasons: {first_result.get('young_researcher_reasons', 'MISSING')}")
            logger.info(f"  - ã‚­ãƒ¼ãƒªã‚¹ãƒˆ: {list(first_result.keys())}")
        return results
    except Exception as e:
        logger.error(f"âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        raise

async def expand_query_with_llm(query: str) -> Dict[str, Any]:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        logger.info(f"ğŸ¤– LLMã‚¯ã‚¨ãƒªæ‹¡å¼µé–‹å§‹: {query}")
        try:
            model = GenerativeModel("gemini-2.0-flash-001")
            prompt = f"""ã‚ãªãŸã¯å­¦è¡“ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸã€Œå…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã«ã¤ã„ã¦ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ç ”ç©¶æƒ…å ±ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ã€ é–¢é€£æ€§ã®é«˜ã„é¡ç¾©èªã€ä¸Šä½/ä¸‹ä½æ¦‚å¿µèªã€è‹±èªã®å¯¾å¿œèªï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰ã€å…·ä½“çš„ãªæŠ€è¡“åã‚„ç‰©è³ªåãªã©ã‚’è€ƒæ…®ã—ã€ æ¤œç´¢ã«æœ‰åŠ¹ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æœ€å¤§10å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚ ææ¡ˆã¯æ—¥æœ¬èªã®å˜èªã¾ãŸã¯çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã§ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è‡ªä½“ã‚‚ææ¡ˆã«å«ã‚ã¦ãã ã•ã„ã€‚ å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œ{query}ã€ ææ¡ˆ:"""
            response = model.generate_content(prompt, generation_config={ "temperature": 0.2, "max_output_tokens": 200, "top_p": 0.8, "top_k": 40 })
            expanded_text = response.text.strip()
            if expanded_text:
                expanded_keywords = [kw.strip() for kw in expanded_text.split(',') if kw.strip()]
                final_keywords = []
                if query not in expanded_keywords: final_keywords.append(query)
                for kw in expanded_keywords:
                    if kw not in final_keywords: final_keywords.append(kw)
                logger.info(f"âœ… LLMã‚¯ã‚¨ãƒªæ‹¡å¼µå®Œäº† (gemini-2.0-flash-001): {final_keywords}")
                return { "original_query": query, "expanded_keywords": final_keywords, "expanded_query": ' '.join(final_keywords[:5]) }
        except Exception as e:
            logger.warning(f"âš ï¸ Gemini 2.0 Flashå¤±æ•—: {e}")
            try:
                model = TextGenerationModel.from_pretrained("text-bison@002")
                prompt = f"""ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹å­¦è¡“ç”¨èªã‚’5-10å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {query} é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:"""
                response = model.predict(prompt, temperature=0.2, max_output_tokens=200, top_p=0.8, top_k=40)
                expanded_text = response.text.strip()
                if expanded_text:
                    expanded_keywords = [kw.strip() for kw in expanded_text.split(',') if kw.strip()]
                    final_keywords = [query] if query not in expanded_keywords else []
                    final_keywords.extend([kw for kw in expanded_keywords if kw not in final_keywords])
                    logger.info(f"âœ… LLMã‚¯ã‚¨ãƒªæ‹¡å¼µå®Œäº† (text-bison@002): {final_keywords}")
                    return { "original_query": query, "expanded_keywords": final_keywords, "expanded_query": ' '.join(final_keywords[:5]) }
            except Exception as e2: logger.warning(f"âš ï¸ Text-Bison ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: {e2}")
        logger.warning("âš ï¸ ã™ã¹ã¦ã®LLMãƒ¢ãƒ‡ãƒ«ã§ã‚¯ã‚¨ãƒªæ‹¡å¼µã«å¤±æ•—")
        return { "original_query": query, "expanded_keywords": [query], "expanded_query": query }
    except Exception as e:
        logger.error(f"âŒ LLMã‚¯ã‚¨ãƒªæ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
        return { "original_query": query, "expanded_keywords": [query], "expanded_query": query }

async def add_llm_summaries(results: List[Dict], query: str) -> List[Dict]:
    # (ã“ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        logger.info(f"ğŸ¤– LLMè¦ç´„ç”Ÿæˆé–‹å§‹: {len(results)}åã®ç ”ç©¶è€…")
        model, model_name = None, ""
        try:
            model = GenerativeModel("gemini-2.0-flash-lite-001")
            model_name = "gemini-2.0-flash-lite-001"
            logger.info(f"âœ… è»½é‡LLMãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ä½¿ç”¨")
        except Exception as e:
            logger.warning(f"âš ï¸ Gemini 2.0 Flash Liteå¤±æ•—: {e}")
            try:
                model = TextGenerationModel.from_pretrained("text-bison@002")
                model_name = "text-bison@002"
                logger.info(f"âœ… LLMãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ä½¿ç”¨")
            except Exception as e2:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«å¤±æ•—: {e2}")
                return results
        if not model:
            logger.error("âŒ åˆ©ç”¨å¯èƒ½ãªLLMãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return results
        for idx, result in enumerate(results):
            try:
                if idx > 0: time.sleep(0.5)
                name, affiliation, keywords, fields, profile, paper, project = result.get('name_ja', ''), result.get('main_affiliation_name_ja', ''), result.get('research_keywords_ja', ''), result.get('research_fields_ja', ''), str(result.get('profile_ja', ''))[:300], result.get('paper_title_ja_first', ''), result.get('project_title_ja_first', '')
                prompt = f"""ç ”ç©¶è€…æƒ…å ±:\nåå‰: {name} ({affiliation})\nç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}\nç ”ç©¶åˆ†é‡: {fields}\nãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {profile}\nä¸»è¦è«–æ–‡: {paper}\nä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project}\n\næ¤œç´¢ã‚¯ã‚¨ãƒª: ã€Œ{query}ã€\n\nä¸Šè¨˜ã®ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€ä¸»è¦è«–æ–‡ã€ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¸ã¾ãˆã¦ã€ ã“ã®ç ”ç©¶è€…ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£æ€§ã‚’200å­—ç¨‹åº¦ã§åˆ†æã—ã¦ãã ã•ã„ã€‚"""
                summary = ""
                if "gemini" in model_name:
                    response = model.generate_content(prompt, generation_config={ "temperature": 0.1, "max_output_tokens": 200, "top_p": 0.8 })
                    summary = response.text.strip()
                else:
                    response = model.predict(prompt, temperature=0.1, max_output_tokens=200, top_p=0.8)
                    summary = response.text.strip()
                if summary: result["llm_summary"] = summary
                else: result["llm_summary"] = f"ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚"
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "Resource exhausted" in error_msg:
                    logger.warning(f"âš ï¸ APIåˆ¶é™ã®ãŸã‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ— ({result.get('name_ja', 'N/A')}): {e}")
                    result["llm_summary"] = "âš ï¸ APIåˆ¶é™ã®ãŸã‚è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ"
                else:
                    logger.warning(f"âš ï¸ å€‹åˆ¥LLMè¦ç´„ã‚¨ãƒ©ãƒ¼ ({result.get('name_ja', 'N/A')}): {e}")
                    result["llm_summary"] = f"ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚"
        logger.info("âœ… LLMè¦ç´„ç”Ÿæˆå®Œäº†")
        return results
    except Exception as e:
        logger.error(f"âŒ LLMè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return results