"""
ç ”ç©¶è€…è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
æ±ç”¨çš„ãªè©•ä¾¡è»¸ã«åŸºã¥ã„ã¦ç ”ç©¶è€…ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã®é–¢é€£æ€§ã‚’è©•ä¾¡
"""

import logging
import json
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from vertexai.generative_models import GenerativeModel
from vertexai.language_models import TextGenerationModel

logger = logging.getLogger(__name__)

@dataclass
class EvaluationCriteria:
    """è©•ä¾¡åŸºæº–ã®å®šç¾©"""
    keyword_match: float = 0.25
    research_directness: float = 0.20
    expertise_depth: float = 0.15
    practical_evidence: float = 0.15
    research_quality: float = 0.10
    interdisciplinary: float = 0.10
    recency: float = 0.05

@dataclass
class ResearcherEvaluation:
    """ç ”ç©¶è€…ã®è©•ä¾¡çµæœ"""
    researcher_data: Dict[str, Any]
    scores: Dict[str, float]
    total_score: float
    summary: str
    strengths: List[str]
    score_reasons: Optional[Dict[str, str]] = None
    
class UniversalResearchEvaluator:
    """æ±ç”¨çš„ãªç ”ç©¶è€…è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.criteria = EvaluationCriteria()
        self.model = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        try:
            # Gemini 2.0 Flash Liteã‚’å„ªå…ˆ
            self.model = GenerativeModel("gemini-2.0-flash-lite-001")
            self.model_name = "gemini-2.0-flash-lite-001"
            logger.info(f"âœ… è©•ä¾¡ç”¨LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {self.model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Gemini 2.0 Flash LiteåˆæœŸåŒ–å¤±æ•—: {e}")
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.model = TextGenerationModel.from_pretrained("text-bison@002")
                self.model_name = "text-bison@002"
                logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {self.model_name}")
            except Exception as e2:
                logger.error(f"âŒ LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e2}")
                self.model = None
    
    async def evaluate_researchers(
        self, 
        researchers: List[Dict[str, Any]], 
        query: str,
        use_internal_evaluation: bool = True
    ) -> List[ResearcherEvaluation]:
        """
        ç ”ç©¶è€…ãƒªã‚¹ãƒˆã‚’è©•ä¾¡
        
        Args:
            researchers: ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            use_internal_evaluation: å†…éƒ¨è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            è©•ä¾¡çµæœã®ãƒªã‚¹ãƒˆ
        """
        if not use_internal_evaluation:
            # å¾“æ¥ã®è©•ä¾¡æ–¹å¼ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            return self._legacy_evaluate(researchers, query)
        
        logger.info(f"ğŸ¯ å†…éƒ¨è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰é–‹å§‹: {len(researchers)}åã®ç ”ç©¶è€…ã‚’è©•ä¾¡")
        
        evaluations = []
        
        # ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡åŒ–ï¼ˆ5äººãšã¤ï¼‰
        batch_size = 5
        for i in range(0, len(researchers), batch_size):
            batch = researchers[i:i+batch_size]
            batch_evaluations = await self._evaluate_batch(batch, query)
            evaluations.extend(batch_evaluations)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            if i + batch_size < len(researchers):
                time.sleep(0.5)
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        evaluations.sort(key=lambda x: x.total_score, reverse=True)
        
        logger.info(f"âœ… è©•ä¾¡å®Œäº†: æœ€é«˜ã‚¹ã‚³ã‚¢ {evaluations[0].total_score:.1f}/10")
        
        return evaluations
    
    async def _evaluate_batch(
        self, 
        researchers: List[Dict[str, Any]], 
        query: str
    ) -> List[ResearcherEvaluation]:
        """ç ”ç©¶è€…ã®ãƒãƒƒãƒã‚’è©•ä¾¡"""
        if not self.model:
            # LLMãŒä½¿ãˆãªã„å ´åˆã¯ç°¡æ˜“è©•ä¾¡
            return [self._simple_evaluate(r, query) for r in researchers]
        
        prompt = self._create_batch_evaluation_prompt(researchers, query)
        
        try:
            if "gemini" in self.model_name:
                response = self.model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0.1,
                        "max_output_tokens": 2048,
                        "top_p": 0.8
                    }
                )
                evaluation_text = response.text
            else:
                response = self.model.predict(
                    prompt,
                    temperature=0.1,
                    max_output_tokens=2048,
                    top_p=0.8
                )
                evaluation_text = response.text
            
            # JSONå½¢å¼ã®è©•ä¾¡çµæœã‚’ãƒ‘ãƒ¼ã‚¹
            evaluations = self._parse_evaluation_response(evaluation_text, researchers, query)
            return evaluations
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡æ˜“è©•ä¾¡ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return [self._simple_evaluate(r, query) for r in researchers]
    
    def _create_batch_evaluation_prompt(
        self, 
        researchers: List[Dict[str, Any]], 
        query: str
    ) -> str:
        """ãƒãƒƒãƒè©•ä¾¡ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        researchers_info = []
        for idx, r in enumerate(researchers):
            info = f"""
ç ”ç©¶è€…{idx + 1}:
åå‰: {r.get('name_ja', '')}
æ‰€å±: {r.get('main_affiliation_name_ja', '')}
ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {r.get('research_keywords_ja', '')}
ç ”ç©¶åˆ†é‡: {r.get('research_fields_ja', '')}
ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {str(r.get('profile_ja', ''))[:300] if r.get('profile_ja') else ''}
ä¸»è¦è«–æ–‡: {r.get('paper_title_ja_first', '')}
ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {r.get('project_title_ja_first', '')}
"""
            researchers_info.append(info)
        
        prompt = f"""ä»¥ä¸‹ã®ç ”ç©¶è€…ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

{chr(10).join(researchers_info)}

å„ç ”ç©¶è€…ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®7ã¤ã®è¦³ç‚¹ã§1-10ç‚¹ã§è©•ä¾¡ã—ã€å„ã‚¹ã‚³ã‚¢ã®ç†ç”±ã‚’å«ã‚ã¦JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

1. keyword_match: ã‚¯ã‚¨ãƒªã¨ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´åº¦
2. research_directness: ç ”ç©¶å†…å®¹ã¨ã‚¯ã‚¨ãƒªã®ç›´æ¥çš„é–¢é€£æ€§
3. expertise_depth: è©²å½“åˆ†é‡ã§ã®å°‚é–€æ€§ã®æ·±ã•
4. practical_evidence: å…·ä½“çš„ãªå®Ÿç¸¾ãƒ»ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
5. research_quality: ç ”ç©¶ã®è³ªã¨å½±éŸ¿åŠ›
6. interdisciplinary: å­¦éš›æ€§ãƒ»å¿œç”¨å¯èƒ½æ€§
7. recency: ç ”ç©¶ã®æœ€æ–°æ€§

å‡ºåŠ›å½¢å¼:
{{
  "evaluations": [
    {{
      "researcher_index": 1,
      "scores": {{
        "keyword_match": 8,
        "research_directness": 9,
        "expertise_depth": 7,
        "practical_evidence": 8,
        "research_quality": 7,
        "interdisciplinary": 6,
        "recency": 8
      }},
      "score_reasons": {{
        "keyword_match": "ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã€{query}ã€ãŒç›´æ¥å«ã¾ã‚Œã¦ã„ã‚‹",
        "research_directness": "ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒ{query}ã®å®Ÿç”¨åŒ–ã«ç„¦ç‚¹",
        "expertise_depth": "è©²å½“åˆ†é‡ã§10å¹´ä»¥ä¸Šã®ç ”ç©¶å®Ÿç¸¾",
        "practical_evidence": "é–¢é€£ç‰¹è¨±3ä»¶ã€å®Ÿç”¨åŒ–äº‹ä¾‹ã‚ã‚Š",
        "research_quality": "ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã¸ã®æ²è¼‰å®Ÿç¸¾",
        "interdisciplinary": "å·¥å­¦ã¨åŒ»å­¦ã®èåˆç ”ç©¶ã‚’æ¨é€²",
        "recency": "2024å¹´ã«æœ€æ–°ã®ç ”ç©¶æˆæœã‚’ç™ºè¡¨"
      }},
      "summary": "ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€ä¸»è¦è«–æ–‡ã€ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¸ã¾ãˆã¦ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£æ€§ã‚’200å­—ç¨‹åº¦ã§è¦ç´„",
      "strengths": ["å¼·ã¿1", "å¼·ã¿2", "å¼·ã¿3"]
    }}
  ]
}}
"""
        return prompt
    
    def _parse_evaluation_response(
        self, 
        response_text: str, 
        researchers: List[Dict[str, Any]], 
        query: str
    ) -> List[ResearcherEvaluation]:
        """LLMã®è©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        evaluations = []
        
        try:
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æŠ½å‡º
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                parsed = json.loads(json_text)
                
                for eval_data in parsed.get('evaluations', []):
                    idx = eval_data.get('researcher_index', 1) - 1
                    if 0 <= idx < len(researchers):
                        researcher = researchers[idx]
                        scores = eval_data.get('scores', {})
                        
                        # ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                        total_score = self._calculate_total_score(scores)
                        
                        evaluation = ResearcherEvaluation(
                            researcher_data=researcher,
                            scores=scores,
                            total_score=total_score,
                            summary=eval_data.get('summary', ''),
                            strengths=eval_data.get('strengths', []),
                            score_reasons=eval_data.get('score_reasons', {})
                        )
                        evaluations.append(evaluation)
            
        except Exception as e:
            logger.error(f"âŒ è©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ‘ãƒ¼ã‚¹ã§ããªã‹ã£ãŸç ”ç©¶è€…ã¯ç°¡æ˜“è©•ä¾¡
        if len(evaluations) < len(researchers):
            for i, researcher in enumerate(researchers):
                if not any(e.researcher_data == researcher for e in evaluations):
                    evaluations.append(self._simple_evaluate(researcher, query))
        
        return evaluations
    
    def _calculate_total_score(self, scores: Dict[str, float]) -> float:
        """é‡ã¿ä»˜ã‘ã•ã‚ŒãŸç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        weighted_sum = 0
        total_weight = 0
        
        criteria_weights = {
            'keyword_match': self.criteria.keyword_match,
            'research_directness': self.criteria.research_directness,
            'expertise_depth': self.criteria.expertise_depth,
            'practical_evidence': self.criteria.practical_evidence,
            'research_quality': self.criteria.research_quality,
            'interdisciplinary': self.criteria.interdisciplinary,
            'recency': self.criteria.recency
        }
        
        for criterion, weight in criteria_weights.items():
            score = scores.get(criterion, 5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ç‚¹
            weighted_sum += score * weight
            total_weight += weight
        
        if total_weight > 0:
            return round(weighted_sum / total_weight, 1)
        return 5.0
    
    def _simple_evaluate(
        self, 
        researcher: Dict[str, Any], 
        query: str
    ) -> ResearcherEvaluation:
        """LLMã‚’ä½¿ã‚ãªã„ç°¡æ˜“è©•ä¾¡"""
        query_lower = query.lower()
        keywords = (researcher.get('research_keywords_ja', '') or '').lower()
        fields = (researcher.get('research_fields_ja', '') or '').lower()
        profile = (researcher.get('profile_ja', '') or '').lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        keyword_match = sum(1 for word in query_lower.split() if word in keywords)
        field_match = sum(1 for word in query_lower.split() if word in fields)
        profile_match = sum(1 for word in query_lower.split() if word in profile)
        
        # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {
            'keyword_match': min(10, 3 + keyword_match * 2),
            'research_directness': min(10, 5 + field_match),
            'expertise_depth': 5,  # ä¸æ˜
            'practical_evidence': 5,  # ä¸æ˜
            'research_quality': 5,  # ä¸æ˜
            'interdisciplinary': 5,  # ä¸æ˜
            'recency': 5  # ä¸æ˜
        }
        
        total_score = self._calculate_total_score(scores)
        
        return ResearcherEvaluation(
            researcher_data=researcher,
            scores=scores,
            total_score=total_score,
            summary=f"ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚",
            strengths=[]
        )
        
    async def generate_single_summary(self, researcher_data: Dict[str, Any], query: str) -> Optional[str]:
        """
        å˜ä¸€ã®ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã‹ã‚‰é–¢é€£æ€§è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹
        """
        # æœ€åˆã«LLMãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if not self.model:
            logger.warning("LLM not available for single summary generation.")
            return "LLMãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã€LLMã‚’å‘¼ã³å‡ºã™
        try:
            prompt = self._create_single_summary_prompt(researcher_data, query)
            
            logger.info(f"å˜ç‹¬è¦ç´„ç”Ÿæˆã®ãŸã‚LLMã‚’å‘¼ã³å‡ºã—: {researcher_data.get('name_ja')} (Query: {query})")
            
            if "gemini" in self.model_name:
                response = self.model.generate_content(prompt, generation_config={"temperature": 0.2})
                summary = response.text
            else: # Fallback for text-bison
                response = self.model.predict(prompt, temperature=0.2)
                summary = response.text
            
            return summary.strip()

        except Exception as e:
            logger.error(f"âŒ å˜ç‹¬è¦ç´„ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _create_single_summary_prompt(self, researcher: Dict[str, Any], query: str) -> str:
        """å˜ç‹¬è¦ç´„ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        info = f"""
ç ”ç©¶è€…æƒ…å ±:
åå‰: {researcher.get('name_ja', '')}
æ‰€å±: {researcher.get('main_affiliation_name_ja', '')}
ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {researcher.get('research_keywords_ja', '')}
ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {str(researcher.get('profile_ja', ''))[:400] if researcher.get('profile_ja') else ''}
ä¸»è¦è«–æ–‡: {researcher.get('paper_title_ja_first', '')}
"""
        prompt = f"""
ä»¥ä¸‹ã®ç ”ç©¶è€…æƒ…å ±ã‚’åŸºã«ã€ç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã€ä¸»è¦è«–æ–‡ã€ä¸»è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¸ã¾ãˆã¦ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã¨ã®é–¢é€£æ€§ã‚’200å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚è¦ç´„æ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

{info}
"""
        return prompt

    def _legacy_evaluate(
        self, 
        researchers: List[Dict[str, Any]], 
        query: str
    ) -> List[ResearcherEvaluation]:
        """å¾“æ¥ã®è©•ä¾¡æ–¹å¼ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰"""
        # æ—¢å­˜ã®llm_summaryã‚’ä½¿ç”¨
        evaluations = []
        for researcher in researchers:
            # ç°¡æ˜“çš„ãªã‚¹ã‚³ã‚¢è¨ˆç®—
            relevance_score = researcher.get('relevance_score', 0)
            distance = researcher.get('distance', 1.0)
            
            if relevance_score:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å ´åˆ
                total_score = min(10, 3 + (relevance_score / 10))
            else:
                # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å ´åˆ
                similarity = 1 - distance
                total_score = round(similarity * 10, 1)
            
            evaluation = ResearcherEvaluation(
                researcher_data=researcher,
                scores={},  # è©³ç´°ã‚¹ã‚³ã‚¢ãªã—
                total_score=total_score,
                summary=researcher.get('llm_summary', ''),
                strengths=[]
            )
            evaluations.append(evaluation)
        
        evaluations.sort(key=lambda x: x.total_score, reverse=True)
        return evaluations
    
    def format_for_ui(
        self, 
        evaluations: List[ResearcherEvaluation], 
        max_results: int = 10
    ) -> Dict[str, Any]:
        """UIè¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        top_evaluations = evaluations[:max_results]
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        if top_evaluations:
            best_match = top_evaluations[0]
            summary = {
                "best_match": f"{best_match.researcher_data.get('name_ja', '')}ï¼ˆ{best_match.researcher_data.get('main_affiliation_name_ja', '')}ï¼‰- ç·åˆã‚¹ã‚³ã‚¢ {best_match.total_score}/10",
                "key_finding": f"ä¸Šä½{min(3, len(top_evaluations))}åã®å¹³å‡ã‚¹ã‚³ã‚¢ã¯{sum(e.total_score for e in top_evaluations[:3]) / min(3, len(top_evaluations)):.1f}ç‚¹ã€‚{best_match.summary}",
                "total_evaluated": len(evaluations)
            }
        else:
            summary = {
                "best_match": "è©²å½“ã™ã‚‹ç ”ç©¶è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                "key_finding": "",
                "total_evaluated": 0
            }
        
        # çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        results = []
        for idx, evaluation in enumerate(top_evaluations):
            result = {
                "rank": idx + 1,
                "name": evaluation.researcher_data.get('name_ja', ''),
                "affiliation": evaluation.researcher_data.get('main_affiliation_name_ja', ''),
                "score": evaluation.total_score,
                "summary": evaluation.summary,
                "strengths": evaluation.strengths[:3],
                "keywords": evaluation.researcher_data.get('research_keywords_ja', ''),
                "fields": evaluation.researcher_data.get('research_fields_ja', ''),
                "profile": evaluation.researcher_data.get('profile_ja', ''),
                "paper_title": evaluation.researcher_data.get('paper_title_ja_first', ''),
                "project_title": evaluation.researcher_data.get('project_title_ja_first', ''),
                "url": evaluation.researcher_data.get('researchmap_url', ''),
                "is_young_researcher": evaluation.researcher_data.get('is_young_researcher', False),
                "young_researcher_reasons": evaluation.researcher_data.get('young_researcher_reasons', [])
            }
            
            # è©³ç´°ã‚¹ã‚³ã‚¢ï¼ˆå†…éƒ¨è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿ï¼‰
            if evaluation.scores:
                result["detail_scores"] = evaluation.scores
                
            # ã‚¹ã‚³ã‚¢ã®ç†ç”±
            if evaluation.score_reasons:
                result["score_reasons"] = evaluation.score_reasons
            
            results.append(result)
        
        return {
            "summary": summary,
            "results": results,
            "metadata": {
                "total_found": len(evaluations),
                "displayed": len(results)
            }
        }
