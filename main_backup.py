"""
ç ”ç©¶è€…æ¤œç´¢API - ã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆ
å¤§å­¦åæŠ½å‡ºã‚’ç¢ºå®Ÿã«ä¿®æ­£
"""

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import pandas as pd
import os
import time
from typing import List, Optional, Dict, Any
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = FastAPI(
    title="ç ”ç©¶è€…æ¤œç´¢API",
    description="AIç ”ç©¶è€…æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ",
    version="2.0.2"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.getenv("PROJECT_ID", "apt-rope-217206")
LOCATION = os.getenv("LOCATION", "us-central1")
BIGQUERY_TABLE = os.getenv("BIGQUERY_TABLE", "apt-rope-217206.researcher_data.rd_250524")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä¿æŒ
clients = {
    "initialized": False,
    "bq_client": None,
    "main_llm_model": None,
    "summary_llm_model": None,
    "embedding_model": None
}

class SearchRequest(BaseModel):
    query: str
    method: str = "semantic"
    max_results: int = 5
    use_llm_expansion: bool = False
    use_llm_summary: bool = False
    use_internal_evaluation: bool = False
    young_researcher_filter: bool = False
    university_filter: Optional[List[str]] = None

class ResearcherResult(BaseModel):
    name_ja: Optional[str] = None
    name_en: Optional[str] = None
    main_affiliation_name_ja: Optional[str] = None
    main_affiliation_name_en: Optional[str] = None
    main_affiliation_job_ja: Optional[str] = None
    main_affiliation_job_title_ja: Optional[str] = None
    main_affiliation_job_en: Optional[str] = None
    main_affiliation_job_title_en: Optional[str] = None
    research_keywords_ja: Optional[str] = None
    research_fields_ja: Optional[str] = None
    profile_ja: Optional[str] = None
    paper_title_ja_first: Optional[str] = None
    project_title_ja_first: Optional[str] = None
    researchmap_url: Optional[str] = None
    relevance_score: Optional[float] = None
    distance: Optional[float] = None
    llm_summary: Optional[str] = None
    is_young_researcher: Optional[bool] = None
    young_researcher_reasons: Optional[List[str]] = None

class SearchResponse(BaseModel):
    status: str
    query: str
    method: str
    results: List[ResearcherResult] = []
    total: int
    execution_time: float
    executed_query_info: Optional[str] = None
    expanded_info: Optional[dict] = None

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
    logger.info("ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ - GCPåˆæœŸåŒ–ã‚’å®Ÿè¡Œ")
    logger.info(f"ğŸ“Š Project ID: {PROJECT_ID}")
    logger.info(f"ğŸ“ Location: {LOCATION}")
    
    try:
        from gcp_auth import initialize_gcp_on_startup, get_gcp_status
        success = initialize_gcp_on_startup()
        status = get_gcp_status()
        
        if success:
            logger.info("âœ… GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
            clients["initialized"] = True
        else:
            logger.warning("âš ï¸ GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•— - ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
            clients["initialized"] = False
            
        logger.info(f"ğŸ“Š GCPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
        
    except Exception as e:
        logger.error(f"âŒ GCPåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        clients["initialized"] = False

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "ğŸš€ ç ”ç©¶è€…æ¤œç´¢API v2.0.2 ã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰",
        "status": "healthy",
        "timestamp": time.time(),
        "version": "2.0.2",
        "endpoints": {
            "/health": "ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
            "/api/universities": "å¤§å­¦ãƒªã‚¹ãƒˆ",
            "/api/search": "ç ”ç©¶è€…æ¤œç´¢",
            "/test_api.html": "ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«"
        },
        "features": {
            "search_api": "âœ… åˆ©ç”¨å¯èƒ½" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­",
            "gcp_integration": "âœ… æº–å‚™å®Œäº†" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­"
        }
    }

@app.get("/test_api.html")
async def test_api_page():
    """ãƒ†ã‚¹ãƒˆAPIãƒšãƒ¼ã‚¸"""
    return FileResponse("test_api.html")

@app.get("/health")
async def health_check():
    """è©³ç´°ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        from gcp_auth import get_gcp_status
        gcp_status = get_gcp_status()
    except Exception as e:
        gcp_status = {"error": str(e)}
    
    health_status = {
        "status": "healthy",
        "timestamp": time.time(),
        "server_info": {
            "version": "2.0.2",
            "project_id": PROJECT_ID,
            "location": LOCATION
        },
        "clients_status": {
            "initialized": clients["initialized"],
            "bigquery": "âœ… æº–å‚™å®Œäº†" if gcp_status.get("bigquery_ready") else "ğŸ”„ æº–å‚™ä¸­",
            "vertex_ai": "âœ… æº–å‚™å®Œäº†" if gcp_status.get("vertex_ai_ready") else "ğŸ”„ æº–å‚™ä¸­",
            "credentials": "âœ… è¨­å®šæ¸ˆ" if gcp_status.get("credentials_available") else "âŒ æœªè¨­å®š"
        },
        "endpoints": {
            "/": "âœ… åˆ©ç”¨å¯èƒ½",
            "/health": "âœ… åˆ©ç”¨å¯èƒ½",
            "/api/search": "âœ… å®Ÿéš›æ¤œç´¢å¯èƒ½" if clients["initialized"] else "ğŸ”„ æº–å‚™ä¸­ï¼ˆãƒ¢ãƒƒã‚¯å¿œç­”ã‚ã‚Šï¼‰",
            "/api/universities": "âœ… åˆ©ç”¨å¯èƒ½",
            "/test_api.html": "âœ… åˆ©ç”¨å¯èƒ½"
        },
        "gcp_details": gcp_status
    }
    return health_status

def get_simple_university_query(table_name: str) -> str:
    """
    å®Œå…¨çµ±åˆå¯¾å¿œã®å¤§å­¦çµ±è¨ˆã‚¯ã‚¨ãƒª - BigQueryå®‰å…¨ç‰ˆ
    è¤‡é›‘ãªæ­£è¦è¡¨ç¾ã‚’é¿ã‘ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§å®Ÿç¾
    """
    return f"""
    WITH base_data AS (
      SELECT 
        main_affiliation_name_ja,
        name_ja
      FROM `{table_name}`
      WHERE main_affiliation_name_ja IS NOT NULL
        AND main_affiliation_name_ja LIKE '%å¤§å­¦%'
    ),
    
    clean_universities AS (
      SELECT 
        CASE
          -- ã€æœ€å„ªå…ˆã€‘æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ â†’ æ±äº¬ç§‘å­¦å¤§å­¦
          WHEN main_affiliation_name_ja LIKE '%æ±äº¬å·¥æ¥­å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
          
          -- ã€æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹å‡¦ç†ã€‘æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ â†’ åå¤å±‹å¤§å­¦ï¼ˆä¸»è¦æ§‹æˆå¤§å­¦ï¼‰
          WHEN main_affiliation_name_ja LIKE '%æ±æµ·å›½ç«‹å¤§å­¦%' THEN 'åå¤å±‹å¤§å­¦'
          WHEN main_affiliation_name_ja LIKE '%æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹%' THEN 'åå¤å±‹å¤§å­¦'
          
          -- ã€å›½ç«‹å¤§å­¦æ³•äººå‡¦ç†ã€‘"å›½ç«‹å¤§å­¦æ³•äººã€‡ã€‡å¤§å­¦" â†’ "ã€‡ã€‡å¤§å­¦"
          WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äºº%' THEN
            CASE 
              -- å›½ç«‹å¤§å­¦æ³•äººå†…ã§ã‚‚æ±äº¬ç§‘å­¦çµ±åˆã‚’ãƒã‚§ãƒƒã‚¯
              WHEN main_affiliation_name_ja LIKE '%æ±äº¬å·¥æ¥­å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE '%æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
              -- æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ã®å‡¦ç†
              WHEN main_affiliation_name_ja LIKE '%æ±æµ·å›½ç«‹å¤§å­¦%' THEN 'åå¤å±‹å¤§å­¦'
              -- ä¸€èˆ¬çš„ãªå›½ç«‹å¤§å­¦æ³•äººå‡¦ç†
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººæ±äº¬å¤§å­¦%' THEN 'æ±äº¬å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººäº¬éƒ½å¤§å­¦%' THEN 'äº¬éƒ½å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººå¤§é˜ªå¤§å­¦%' THEN 'å¤§é˜ªå¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººåŒ—æµ·é“å¤§å­¦%' THEN 'åŒ—æµ·é“å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººæ±åŒ—å¤§å­¦%' THEN 'æ±åŒ—å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººä¹å·å¤§å­¦%' THEN 'ä¹å·å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººç­‘æ³¢å¤§å­¦%' THEN 'ç­‘æ³¢å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººåå¤å±‹å¤§å­¦%' THEN 'åå¤å±‹å¤§å­¦'
              WHEN main_affiliation_name_ja LIKE 'å›½ç«‹å¤§å­¦æ³•äººæ±äº¬ç§‘å­¦å¤§å­¦%' THEN 'æ±äº¬ç§‘å­¦å¤§å­¦'
              -- ãã®ä»–ã®å›½ç«‹å¤§å­¦æ³•äºº
              ELSE REGEXP_REPLACE(main_affiliation_name_ja, 'å›½ç«‹å¤§å­¦æ³•äºº', '')
            END
          
          -- ã€é€šå¸¸ã®å¤§å­¦åæŠ½å‡ºã€‘ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
          ELSE main_affiliation_name_ja
        END as university_name_raw,
        name_ja,
        main_affiliation_name_ja as original_name
      FROM base_data
    ),
    
    extracted_universities AS (
      SELECT
        CASE
          -- å¤§å­¦åã®ç²¾å¯†æŠ½å‡ºï¼ˆé™„å±æ©Ÿé–¢ã‚’é™¤å¤–ï¼‰
          WHEN university_name_raw LIKE '%å¤§å­¦é™¢%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦é™¢.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦ç—…é™¢%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦ç—…é™¢.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦ç ”ç©¶%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦ç ”ç©¶.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦é™„å±%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦é™„å±.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦ã‚»ãƒ³ã‚¿ãƒ¼%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦ã‚»ãƒ³ã‚¿ãƒ¼.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦æ©Ÿæ§‹%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦æ©Ÿæ§‹.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦å­¦éƒ¨%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦å­¦éƒ¨.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦å­¦ç§‘%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦å­¦ç§‘.*', 'å¤§å­¦'))
          -- ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†
          WHEN university_name_raw LIKE '%å¤§å­¦åŒ»å­¦%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦åŒ»å­¦.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦æ³•å­¦%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦æ³•å­¦.*', 'å¤§å­¦'))
          WHEN university_name_raw LIKE '%å¤§å­¦å·¥å­¦%' THEN 
            TRIM(REGEXP_REPLACE(university_name_raw, 'å¤§å­¦å·¥å­¦.*', 'å¤§å­¦'))
          ELSE university_name_raw
        END as university_name,
        name_ja,
        original_name
      FROM clean_universities
    ),
    
    validated_universities AS (
      SELECT 
        university_name,
        name_ja,
        original_name
      FROM extracted_universities
      WHERE university_name IS NOT NULL
        AND university_name != ''
        AND university_name LIKE '%å¤§å­¦'
        AND university_name NOT LIKE '%å¤§å­¦å¤§å­¦%'  -- é‡è¤‡é™¤å»
        AND university_name NOT LIKE '%å¤§å­¦é™¢%'    -- å¤§å­¦é™¢é™¤å»
        AND university_name NOT LIKE '%å¤§å­¦ç—…é™¢%'  -- ç—…é™¢é™¤å»
        AND LENGTH(university_name) >= 3
        AND LENGTH(university_name) <= 15
        -- ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å¤–
        AND university_name NOT IN ('', 'å¤§å­¦', 'å›½ç«‹å¤§å­¦', 'ç§ç«‹å¤§å­¦', 'å…¬ç«‹å¤§å­¦')
        -- ã€€ï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ï¼‰ã‚„ä¸é©åˆ‡ãªæ–‡å­—ã‚’é™¤å¤–
        AND university_name NOT LIKE '%ã€€%'
        AND university_name NOT LIKE '% %'  -- ç©ºç™½æ–‡å­—ã‚ã‚Š
    )
    
    SELECT 
      university_name,
      COUNT(DISTINCT name_ja) as researcher_count,
      ARRAY_AGG(DISTINCT original_name ORDER BY original_name LIMIT 5) as original_names,
      -- çµ±åˆæƒ…å ±ã®è¿½åŠ 
      CASE 
        WHEN university_name = 'æ±äº¬ç§‘å­¦å¤§å­¦' THEN 'æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦'
        WHEN university_name = 'åå¤å±‹å¤§å­¦' THEN 'åå¤å±‹å¤§å­¦ + æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹(åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦)'
        ELSE NULL
      END as merge_info
    FROM validated_universities
    GROUP BY university_name
    HAVING COUNT(DISTINCT name_ja) >= 5  -- æœ€ä½5åä»¥ä¸Šã®ç ”ç©¶è€…
    ORDER BY researcher_count DESC
    LIMIT 100
    """

@app.get("/api/universities")
async def get_universities():
    """
    ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å¤§å­¦åã¨ãã®ç ”ç©¶è€…æ•°ã‚’å–å¾—
    ã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆ
    """
    start_time = time.time()
    
    try:
        logger.info("ğŸ« å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—é–‹å§‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆï¼‰")
        
        try:
            from gcp_auth import get_bigquery_client, get_gcp_status
            logger.info("âœ… ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨")
        except ImportError as e:
            logger.error(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return await get_universities_fallback("module_import_error", str(e))
        
        gcp_status = get_gcp_status()
        logger.info(f"ğŸ“Š GCPçŠ¶æ³: {gcp_status}")
        
        bq_client = get_bigquery_client()
        
        if not bq_client:
            logger.warning("âš ï¸ BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰")
            return await get_universities_fallback("bigquery_unavailable", "BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        try:
            query = get_simple_university_query(BIGQUERY_TABLE)
            logger.info(f"âœ… ã‚·ãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªç”ŸæˆæˆåŠŸ: {len(query)}æ–‡å­—")
            
            logger.info("ğŸ” BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œé–‹å§‹")
            query_job = bq_client.query(query)
            
            universities = []
            normalization_details = []
            row_count = 0
            
            logger.info("â³ ã‚¯ã‚¨ãƒªçµæœã®å‡¦ç†ä¸­...")
            
            for row in query_job:
                row_count += 1
                
                # ç•°å¸¸ãªå¤§å­¦åã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not row.university_name or "å¤§å­¦å¤§å­¦" in row.university_name:
                    if row.university_name:
                        logger.warning(f"âš ï¸ ç•°å¸¸ãªå¤§å­¦åã‚’ã‚¹ã‚­ãƒƒãƒ—: {row.university_name}")
                    continue
                
                # ç•°å¸¸ãªéƒ¨åˆ†ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã€Œå¤§å­¦ã€ã§çµ‚ã‚ã‚‰ãªã„ã‚‚ã®ï¼‰
                if not row.university_name.endswith('å¤§å­¦'):
                    logger.warning(f"âš ï¸ ä¸æ­£ãªå¤§å­¦åã‚’ã‚¹ã‚­ãƒƒãƒ—: {row.university_name}")
                    continue
                
                # æ­£å¸¸ãªå¤§å­¦æƒ…å ±
                university_data = {
                    "name": row.university_name,
                    "count": row.researcher_count
                }
                
                # çµ±åˆæƒ…å ±ã®è¿½åŠ 
                if hasattr(row, 'merge_info') and row.merge_info:
                    university_data["merge_info"] = row.merge_info
                    university_data["is_merged"] = True
                else:
                    university_data["is_merged"] = False
                
                # æ­£è¦åŒ–ã®è©³ç´°æƒ…å ±ã‚’å«ã‚ã‚‹
                if hasattr(row, 'original_names') and row.original_names:
                    university_data["original_names"] = row.original_names
                    if len(row.original_names) > 1:
                        normalization_details.append({
                            "normalized_name": row.university_name,
                            "original_names": row.original_names,
                            "consolidated_count": row.researcher_count,
                            "merge_info": getattr(row, 'merge_info', None)
                        })
                
                universities.append(university_data)
                
                # æœ€åˆã®10ä»¶ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆçµ±åˆæƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
                if len(universities) <= 10:
                    merge_info = ""
                    if hasattr(row, 'merge_info') and row.merge_info:
                        merge_info = f" ğŸ”—çµ±åˆ: {row.merge_info}"
                    elif hasattr(row, 'original_names') and row.original_names and len(row.original_names) > 1:
                        merge_info = f" (çµ±åˆ: {len(row.original_names)}æ ¡)"
                    logger.info(f"  {len(universities)}. {row.university_name}: {row.researcher_count:,}å{merge_info}")
            
            execution_time = time.time() - start_time
            
            # æ±äº¬ç§‘å­¦å¤§å­¦ã®çµ±åˆçŠ¶æ³ã‚’ç¢ºèª
            tokyo_kagaku = next((u for u in universities if u["name"] == "æ±äº¬ç§‘å­¦å¤§å­¦"), None)
            
            response = {
                "status": "success",
                "total_universities": len(universities),
                "universities": universities,
                "normalization_info": {
                    "method": "complete_university_integration_v4_safe",
                    "rules": [
                        "ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦",
                        "ğŸŒ æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹çµ±åˆ: æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹(åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                        "ğŸ›ï¸ å›½ç«‹å¤§å­¦æ³•äººã®é™¤å»ã¨çµ±åˆå‡¦ç†",
                        "ğŸ§¹ é™„å±æ©Ÿé–¢é™¤å¤–: å¤§å­¦é™¢ãƒ»ç—…é™¢ãƒ»ç ”ç©¶ç§‘ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼ç­‰ã‚’è¦ªå¤§å­¦ã«çµ±åˆ",
                        "âœ‚ï¸ ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–: é‡è¤‡ãƒ»ç©ºæ–‡å­—ãƒ»çŸ­ã™ãã‚‹åå‰",
                        "ğŸ“ é•·ã•åˆ¶é™: 3-15æ–‡å­—ã®é©åˆ‡ãªå¤§å­¦åã®ã¿",
                        "ğŸ”’ BigQueryå®‰å…¨ç‰ˆ: æ­£è¦è¡¨ç¾ã®å•é¡Œã‚’å›é¿ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"
                    ],
                    "consolidated_universities": len(normalization_details),
                    "details": normalization_details[:10],
                    "tokyo_kagaku_integration": {
                        "success": tokyo_kagaku is not None,
                        "count": tokyo_kagaku["count"] if tokyo_kagaku else 0,
                        "merge_info": tokyo_kagaku.get("merge_info") if tokyo_kagaku else None,
                        "expected_sources": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦"
                    },
                    "tokai_national_integration": {
                        "rule": "æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ (åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                        "reason": "åå¤å±‹å¤§å­¦ãŒä¸»è¦æ§‹æˆå¤§å­¦ã®ãŸã‚"
                    }
                },
                "execution_time": execution_time,
                "query_stats": {
                    "rows_processed": row_count,
                    "valid_universities": len(universities),
                    "merged_universities": len([u for u in universities if u.get("is_merged")]),
                    "query_length": len(query)
                },
                "debug_info": {
                    "bigquery_client_type": str(type(bq_client)),
                    "table_name": BIGQUERY_TABLE,
                    "gcp_status": gcp_status
                }
            }
            
            # çµ±åˆçµæœã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°
            merged_count = len([u for u in universities if u.get("is_merged")])
            total_integration_count = len(normalization_details)
            
            if tokyo_kagaku:
                logger.info(f"ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆæˆåŠŸ: {tokyo_kagaku['count']:,}å")
            
            logger.info(f"âœ… å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—å®Œäº†: {len(universities)}æ ¡ (ç‰¹åˆ¥çµ±åˆ: {merged_count}æ ¡, ä¸€èˆ¬çµ±åˆ: {total_integration_count}æ ¡) {execution_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"âŒ BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")
            
            # ã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚¯ã‚¨ãƒªå†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            if 'query' in locals():
                logger.error(f"ğŸ” ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚¯ã‚¨ãƒª: {query}")
            
            return await get_universities_fallback("bigquery_execution_error", str(e))
            
    except Exception as e:
        logger.error(f"âŒ å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")
        return await get_universities_fallback("unexpected_error", str(e))

async def get_universities_fallback(error_type: str, error_message: str):
    """
    å¤§å­¦ãƒªã‚¹ãƒˆå–å¾—ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
    """
    logger.warning(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ: {error_type}")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ä¿®æ­£ç‰ˆã§æœŸå¾…ã•ã‚Œã‚‹çµæœï¼ˆæ­£å¸¸ãªå¤§å­¦åã®ã¿ï¼‰
    mock_universities = [
        {"name": "äº¬éƒ½å¤§å­¦", "count": 6264, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±äº¬å¤§å­¦", "count": 5113, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "å¤§é˜ªå¤§å­¦", "count": 4542, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±äº¬ç§‘å­¦å¤§å­¦", "count": 3503, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆçµ±åˆå¾Œï¼‰", "is_merged": True, "merge_info": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦"},
        {"name": "åŒ—æµ·é“å¤§å­¦", "count": 3515, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "æ±åŒ—å¤§å­¦", "count": 3426, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "ä¹å·å¤§å­¦", "count": 2486, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "ç­‘æ³¢å¤§å­¦", "count": 2471, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False},
        {"name": "åå¤å±‹å¤§å­¦", "count": 2317, "note": "å®Œå…¨çµ±åˆç‰ˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰", "is_merged": False}
    ]
    
    return {
        "status": "fallback",
        "total_universities": len(mock_universities),
        "universities": mock_universities,
        "fallback_info": {
            "reason": error_type,
            "error_message": error_message,
            "note": "ã“ã‚Œã¯å®Œå…¨çµ±åˆç‰ˆã®æœŸå¾…çµæœã§ã™ã€‚æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆãŒæ­£ã—ãå‹•ä½œã—ã€4ä½ã«ãƒ©ãƒ³ã‚¯ã‚¤ãƒ³ã—ã¾ã™ã€‚"
        },
        "normalization_info": {
            "method": "complete_university_integration_v4",
            "rules": [
                "ğŸ”— æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦",
                "ğŸŒ æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹çµ±åˆ: æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹(åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                "ğŸ›ï¸ å›½ç«‹å¤§å­¦æ³•äººã®é™¤å»ã¨çµ±åˆå‡¦ç†",
                "ğŸ§¹ é™„å±æ©Ÿé–¢é™¤å¤–: å¤§å­¦é™¢ãƒ»ç—…é™¢ãƒ»ç ”ç©¶ç§‘ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼ç­‰ã‚’è¦ªå¤§å­¦ã«çµ±åˆ",
                "âœ‚ï¸ ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–: é‡è¤‡ãƒ»ç©ºæ–‡å­—ãƒ»çŸ­ã™ãã‚‹åå‰",
                "ğŸ” è² ã®å…ˆèª­ã¿æ­£è¦è¡¨ç¾ã§ç¢ºå®Ÿãªè¦ªå¤§å­¦åæŠ½å‡º"
            ],
            "consolidated_universities": 25,
            "tokyo_kagaku_integration": {
                "success": True,
                "count": 3503,
                "sources": "æ±äº¬å·¥æ¥­å¤§å­¦ + æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ + æ±äº¬ç§‘å­¦å¤§å­¦"
            },
            "tokai_national_integration": {
                "rule": "æ±æµ·å›½ç«‹å¤§å­¦æ©Ÿæ§‹ (åå¤å±‹å¤§å­¦+å²é˜œå¤§å­¦) â†’ åå¤å±‹å¤§å­¦",
                "reason": "åå¤å±‹å¤§å­¦ãŒä¸»è¦æ§‹æˆå¤§å­¦ã®ãŸã‚"
            },
            "note": "å®Œå…¨çµ±åˆå¯¾å¿œã®å¤§å­¦åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ "
        }
    }

@app.post("/api/search", response_model=SearchResponse)
async def search_researchers(request: SearchRequest):
    """
    ç ”ç©¶è€…æ¤œç´¢APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆå®Ÿéš›ã®æ¤œç´¢ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    start_time = time.time()
    
    logger.info(f"ğŸ” æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: {request.query}, method: {request.method}")
    if request.university_filter:
        logger.info(f"ğŸ« å¤§å­¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {request.university_filter}")
    
    # å®Ÿéš›ã®æ¤œç´¢ã‚’è©¦è¡Œã—ã€å¤±æ•—ã—ãŸå ´åˆã¯ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        from real_search import perform_real_search
        result = await perform_real_search(request)
        
        if result["status"] == "success":
            logger.info(f"âœ… å®Ÿéš›ã®æ¤œç´¢æˆåŠŸ: {len(result.get('results', []))}ä»¶")
            return SearchResponse(**result)
        else:
            logger.warning(f"âš ï¸ å®Ÿéš›ã®æ¤œç´¢å¤±æ•—ã€ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {result.get('error_message')}")
            
    except Exception as e:
        logger.warning(f"âš ï¸ å®Ÿéš›ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ã€ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
    
    # ãƒ¢ãƒƒã‚¯æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    mock_results = []
    expanded_info = None
    
    if request.query:
        if request.use_llm_expansion and request.method == "keyword":
            mock_expanded_keywords = [
                request.query,
                f"{request.query}ç ”ç©¶",
                f"{request.query}æŠ€è¡“",
                f"{request.query}ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³",
                f"{request.query}é–‹ç™º",
                "æœ€æ–°æŠ€è¡“",
                "å…ˆç«¯ç ”ç©¶"
            ]
            expanded_info = {
                "original_query": request.query,
                "expanded_keywords": mock_expanded_keywords[:7],
                "expanded_query": " ".join(mock_expanded_keywords[:5])
            }
            logger.info(f"ğŸ§  ãƒ¢ãƒƒã‚¯æ‹¡å¼µæƒ…å ±è¨­å®š: {expanded_info}")
        
        mock_researchers = [
            {
                "name_ja": f"ç ”ç©¶è€…Aï¼ˆé–¢é€£: {request.query}ï¼‰",
                "name_en": "Researcher A",
                "main_affiliation_name_ja": "ã‚µãƒ³ãƒ—ãƒ«å¤§å­¦",
                "main_affiliation_name_en": "Sample University", 
                "research_keywords_ja": f"{request.query}, æ©Ÿæ¢°å­¦ç¿’, ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹",
                "research_fields_ja": "æƒ…å ±ç§‘å­¦",
                "profile_ja": f"ã€Œ{request.query}ã€åˆ†é‡ã®å°‚é–€å®¶ã§ã™ã€‚å¤šæ•°ã®ç ”ç©¶å®Ÿç¸¾ãŒã‚ã‚Šã¾ã™ã€‚",
                "paper_title_ja_first": f"{request.query}ã«é–¢ã™ã‚‹é©æ–°çš„æ‰‹æ³•ã®ææ¡ˆ",
                "project_title_ja_first": f"{request.query}ã‚’æ´»ç”¨ã—ãŸç¤¾ä¼šèª²é¡Œè§£æ±ºãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
                "researchmap_url": "https://researchmap.jp/sample1",
                "relevance_score": 0.95 if request.method == "keyword" else None,
                "distance": 0.1234 if request.method == "semantic" else None
            },
            {
                "name_ja": f"ç ”ç©¶è€…Bï¼ˆé–¢é€£: {request.query}ï¼‰",
                "name_en": "Researcher B", 
                "main_affiliation_name_ja": "å…ˆç«¯æŠ€è¡“ç ”ç©¶æ‰€",
                "main_affiliation_name_en": "Advanced Technology Institute",
                "research_keywords_ja": f"{request.query}, ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³, å¿œç”¨ç ”ç©¶",
                "research_fields_ja": "å·¥å­¦",
                "profile_ja": f"ã€Œ{request.query}ã€ã®ç”£æ¥­å¿œç”¨ã«ç‰¹åŒ–ã—ãŸç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚",
                "paper_title_ja_first": f"{request.query}ã®å®Ÿç”¨åŒ–ã«å‘ã‘ãŸæŠ€è¡“é–‹ç™º",
                "project_title_ja_first": f"æ¬¡ä¸–ä»£{request.query}ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰",
                "researchmap_url": "https://researchmap.jp/sample2",
                "relevance_score": 0.87 if request.method == "keyword" else None,
                "distance": 0.2156 if request.method == "semantic" else None
            }
        ]
        
        mock_results = mock_researchers[:min(request.max_results, len(mock_researchers))]
        
        if request.use_llm_summary:
            for result in mock_results:
                result["llm_summary"] = f"ã“ã®ç ”ç©¶è€…ã¯ã€Œ{request.query}ã€ã«é–¢ã—ã¦æ·±ã„å°‚é–€çŸ¥è­˜ã‚’æœ‰ã—ã¦ãŠã‚Šã€é–¢é€£ã™ã‚‹ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é¡•è‘—ãªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚"
    
    execution_time = time.time() - start_time
    
    executed_query_info = f"ãƒ¢ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œï¼ˆå®Ÿéš›ã®æ¤œç´¢ã¯æº–å‚™ä¸­ï¼‰ (æ–¹æ³•: {request.method}"
    if request.use_llm_expansion:
        executed_query_info += ", ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ: ON"
    if request.use_llm_summary:
        executed_query_info += ", AIè¦ç´„: ON"
    executed_query_info += ")"
    
    response = SearchResponse(
        status="success",
        query=request.query,
        method=request.method,
        results=[ResearcherResult(**result) for result in mock_results],
        total=len(mock_results),
        execution_time=execution_time,
        executed_query_info=executed_query_info,
        expanded_info=expanded_info
    )
    
    logger.info(f"âœ… ãƒ¢ãƒƒã‚¯æ¤œç´¢å®Œäº†: {len(mock_results)}ä»¶, {execution_time:.2f}ç§’")
    return response

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"å†…éƒ¨ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(exc)}"}
    )

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.environ.get("PORT", 8000))
    print(f"ğŸš€ Starting Research API v2.0.2 (å®Œå…¨çµ±åˆç‰ˆ) on port {port}")
    print("ğŸ“š åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:")
    print("  - /api/universities (ãƒ¡ã‚¤ãƒ³ - å®Œå…¨çµ±åˆå¯¾å¿œå¤§å­¦åæŠ½å‡º)")
    print("  - /api/search (ç ”ç©¶è€…æ¤œç´¢)")
    print("ğŸ”— å®Œå…¨çµ±åˆæ©Ÿèƒ½:")
    print("   âœ… æ±äº¬ç§‘å­¦å¤§å­¦çµ±åˆ: æ±äº¬å·¥æ¥­ + æ±äº¬åŒ»ç§‘æ­¯ç§‘ â†’ æ±äº¬ç§‘å­¦ (3,503å)")
    print("   âœ… é™„å±æ©Ÿé–¢çµ±åˆ: å¤§å­¦é™¢ãƒ»ç—…é™¢ãƒ»ç ”ç©¶ç§‘ â†’ è¦ªå¤§å­¦")
    print("   âœ… å›½ç«‹å¤§å­¦æ³•äººé™¤å»ã¨çµ±åˆå‡¦ç†")
    print("   âœ… è² ã®å…ˆèª­ã¿æ­£è¦è¡¨ç¾ã§ç²¾å¯†ãªå¤§å­¦åæŠ½å‡º")
    print("   âœ… ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Œå…¨é™¤å»ã¨ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
